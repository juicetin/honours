\chapter{Introduction}

Earth's oceans cover 70\% of its surface, but only less than 10\% of the Earth's oceans have been explored to date\footnote{Oceanservice.noaa.gov. (2016). How much of the ocean have we explored?. [online] Available at: http://oceanservice.noaa.gov/facts/exploration.html}. There have been increasing efforts over the past few decades to more efficiently map out these unexplored areas to monitor marine ecosystems to be able to track the state of them over time for management, preseveration, etc. purposes. The process used is called benthic habitat mapping, which is the process of generating predictive maps of different habitat types at the bottom of a body of water. Most efforts to create benthic habitat maps share some basic key steps - acoustic data is used to estimate properties about the surface of the benthos, then its relationship with other `truthing' data such as images or video are inferred. Early attempts at this understanding and modeling these relationships would have been expert-driven where any map created by an individual or individuals would be fully subject to their personal biases and knowledge. With the lack of better tools available, this was the best option - until machine learning algorithms began to be formulated more formally, and applied to real world problems. Thus, deterministic methods such as random forests and support vector machines made their debut, and enabled predictions on large sets of potentially complex data that would have previously proved to be a considerable challenge or even impossible when done manually, and would have required major compromises. However, considering that the creation of habitat maps would be for conservation of marine management purposes, it would be important to be able to state the confidence of the predictions are made, a property that deterministic methods do not possess. Gaussian processes are a complex model that can provide this information - providing both the probability of every label being the `correct' one at a certain data point, as well as the variance on each of these predictions. Due to the $O(n^3)$ matrix inversion steps involved in them, the time that would be required to fit a Gaussian process to more beyond several thousand points would be astronomical and impractical for any real world applications. To account for this, there exist approximations such as product of experts and Bayesian committee machines that allow levels of parallelism of the model (that are inapplicable to the basic Gaussian process) that are (almost) bounded only by hardware. However, all this ignores the fact that asynchronously collected bathymetry and image data will not correspond to one another in a $1:1$ manner, with each bathymetry point likely related to several images. This results in a dataset where for each coordinate with bathymetry data, the labels across several labels are attributed to it, or in other words, a vector of label counts. As the aforementioned models do not handle such data, approximations were made where these vectors were simplified down to a single label, throwing out a majority of the information that was collected. To actually fully utiliese these vector counts, a method is needed that can model this data appropriately - such as the Dirichlet Multinomial. This allows predictions to be made over category counts, providing normalised predictions at each point, not only correctly interpreting the data in contrast to other methods used in benthic habitat mapping, but also allowing properties of the benthos such as biodiversity and co-habitation to be directly inferred from the data, not requiring additional post-processing steps.

% It is the relationship which is inferred between the different data sets inferred using machine learning techniques that varies between studies. A considerable portion of such studies are shown to use deterministic methods to predict a label for any given coordinate such as Random Forests and Support Vector machines (SVMs), whilst more recent ones make use of more informative methods such as Gaussian Processes, providing a distribution over all possible labels given any data point.

\section{Contribution}

\todo{(need to rewrite most of this. investigating probabilistic methods and improving their performance as well)} The main contribution of this thesis will be to explore how to use data where a single data point does not have only one label exclusively, but instead corresponds to a tally of each possible label. For example, a particular 5m x 5m area in the benthos may be an even mix of both sand and coral, but in previous literature, the data was simplified such that whichever label occurred more frequently regardless of how small the margin would be the single label assigned to that point. This results in a very coarse approximation even when using Gaussian Processes attempts to model the uncertainty/uncertainty with its predictions at each point (but ultimately only provides a single, final prediction). To alleviate this and provide a richer set of information, we explore the use of Dirichlet Multinomials, which provides a distribution of each label that represents something entirely different. Whereas in a Gaussian Process, each label is assigned the probability of being the correct one, the output of a Dirichlet Multinomial Regressor provides the distribution of the frequency of labels in a particular space itself. See section \todo{GP vs DM} for an illustrative example on how results would differ in practice between the two methods.

\section{Motivation}

\todo{(motivation for speeding up GP?)} The motivation behind assessing the effectiveness and advantages of such a method are that they inherently tie in with lower resolution data, particularly when a single bathymetry points corresponds to a large enough area such that one would expect a mix of different labels. This is advantageous because we want to be able to re-sample data from any given site periodically (for example, every 3-4 years) whilst being economically efficient. This naturally lends to lower resolution data, meaning that summarising large areas to a single label would theoretically be throwing away a majority of the information contained in bathymetry and image data.

\section{Outline}

We will first look at the existing literature in \Cref{chap:litreview} - on the collection of bathymetry and image data briefly, then on deterministic approaches to benthic habitat mapping to date, such as logistic regression, and random forests, and their performance on varying types of benthic environments. This is contrast the more informative probabilistic and multi-output approaches that will be explained in \Cref{chap:gps} and \Cref{chap:dms}, where we look at the mathematical background behind Gaussian processes and Dirichlet multinomial regression. In \Cref{chap:experiments}, we then apply the techniques explained in the previous chapters and observe their performance, points of interest, as well as how the information obtained differs to methods visited in \Cref{chap:litreview}. 
