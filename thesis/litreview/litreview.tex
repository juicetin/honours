\chapter{Related Work} \label{chap:relatedwork}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OVERVIEW %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this chapter, an overview of will be provided of what benthic habitat mapping is and the general steps involved in data collection, followed by a review of some of most commonly used approaches when performing benthic habitat mapping. The practice of benthic habitat mapping precedes the rapid developments in machine learning methods in recent history, and as such, early attempts would naturally have involved manual predictions based on available information that would be subject to the biases of experts involved in the process. It is thus expected that given the same raw data, different experts who have had varying experiences in their field would come to different conclusions. 

This phenomena was observed in a geoscience related study \citep{bond07}, where over 400 individuals with geoscience backgrounds were asked to assess a synthesised seismic image, with just under 25\% correctly identifying the `true' tectonic setting and the three main fault strands. Interestingly however, (inexperienced) students were as likely to give incorrect responses as those with over 15 years' experience, where the latter often drew conclusions linked to the area that the held expertise in. Early efforts to create benthic maps dating back to the at least the 1980's\citep{gibson07} followed this trend, where the lack of more formalised approaches meant experts would use the available data to extrapolate habitat maps based on the understanding that they had. This points to the variability of expert-driven modeling of natural environments such as benthic habitats, and the need for data-driven techniques, where expert input can be used as a supporting source of information rather than the only, or dominant one. 

\section{Benthic Habitat Mapping Overview}

The process of benthic habitat mapping involves three key steps that the large majority of all studies in the area go through\citep{ozcoasts}. In this section, a brief overview of each of these steps will be given, along with common procedures involved. Habitat characterisation extracts the physical properties of an environment, whereas classification uses the data available to classify or cluster the raw data into habitat groups. Habitat mapping would then involve the modelling of relationships between the existing data, and in the case where the availability of one source far exceeds another, the relationship determined at the overlapping areas is then extrapolated to where only one data source exists, to formulate a habitat map.

%%%%%%%%%%%%%%% Habitat Characterisation %%%%%%%%%%%%%%%
\subsection{Habitat Characterisation}
If high resolution, multimodal\footnote{Multimodal data refers to the different \textit{information} that resides within it - e.g. an area of benthic terrain can be represented by a single photograph, or numerical representations of its physical properties.} data for the entire ocean's benthos was easily obtainable, creating benthic habitats maps for any given area would be not be an incredibly difficult task. As this is prohibitively expensive, the altnerative is instead collecting relatively large amounts of economically obtainable information such as bathymatery data, and comparatively fewer samples of data that are costly to collect such as images (so that a relationship can be modeled between them, to be explained below). This subsection provides a brief summary of data collected and methods used to do so.

As exhaustively exploring Earth's ocean floor with underwater vehicles to capture it visually as well as all its physical properties is an infeasible task, compromise is required so that modest amounts of data can be collected economically, whilst still being informative. Remote-sensing data is thus used, usually obtained from acoustic backscatter methods, involing the firing of sound waves towards the benthos, where their frequency and strength upon returning is used to deduce the depth of the area from where it rebounded (and in turn, allowing other properties to be inferred, such as slope and rugosity\footnote{rugosity is the measurement of surface roughness}). Shipborne tools such as single-beam echosounders (SBES) and multi-beam echo sounders (MBES) facilitate the collection of this data in the form of acoustic backscatter, with the latter being the modern alternative that can collect larger quantities of higher quality data more efficiently\citep{calvert15}. Since bathymetry data alone is not enough to create habitat maps as this would incorrectly assume that a limited set of physical properties alone can fully explain a habitat~\citep{snelgrove94}, truthing data also needs to be collected to verify habitats. Before underwater vehicles equipped with any number of capabilities required by researchers and the like were as readily available as they are today, collecting sediment samples would be a common method to verify habitats. Given the current technology, images are easier to collect via use of autonomous underwater vehicles (AUVs) that can be more readily processed afterwards as well. 

However, since AUVs operate at the benthos, and methods such as MBES for collecting bathymetry data occurs at the ocean surface, it would be expected that the exact coordinates at which they are taken would not match up exactly. To account for this, in the case where multiple images exist around any given bathymetry point, all these images are then attributed to it, as shown in \Cref{fig:bathy_tour_ex}

\begin{figure}
    \includegraphics[scale=0.75]{bathy_tour_ex.pdf}
    \caption{An example of bathymetry data and image collection. As the density of images are higher than the bathymetry data, the method of utilising both sources fully is that every image is assigned to the closest bathymetry point (within reason), resulting in each of them taking on some number of habitat labels, dependent on the AUV's path.}
    \label{fig:bathy_tour_ex}
\end{figure}

%%%%%%%%%%%%%%% Habitat Classification %%%%%%%%%%%%%%%
\subsection{Habitat Classification}

Almost all studies use \textit{in situ}\footnote{in situ, in a biological context, refers to the precise spot in which something occurs. In a habitat mapping context specifically when referring to truthing data, it simply means, in the case of bathymetry and image data, images taken at the exact spot corresponding to a particular bathymetry sampling location.} data to complement the acoustic data for building a model between these two sources. However, this data needs to be labelled with the habitat they belong to such as `bedrock covered by discontinuous seagrass cover' or `Maerl interspersed with gravel' \citep{micallef12}, with the algorithms for doing so falling under either the supervised or unsupervised categories.

Studies have used both unsupervised clustering~\citep{friedman11} and supervised clustering (classification) to label the truthing data for the model-fitting stage. Often, there may be large amounts of visual data, beyond that which any human or even team can reasonably, manually cluster - and as such, unsupervised algorithms are first used to create these clusters, after which an expert may be brought in to verify/simplify (or otherwise) the resulting clusters. One possible method in the unsupervised category is to use hierarchical clustering as seen in \citet{pizarro09}. This is an approach in which a layered tree\footnote{the tree described here is formally known as a \textit{dendrogram}} is formed where the two nearest clusters (based on a pre-deinfed distance metric) are grouped together to form a larger cluster, with a base case where every point is its own cluster. The distance metric used in this particular study was the Kullback-Leibler convergence between points, where each was attributed to a distribution of a set of features, including properties such as saliency that was calculated using colour and texture of the image, as well as colour histograms of comprehensively normalised images.

Through more complex approaches, the supervised and unsupervised paradigms can be combined to get benefit from the advantages of both - reducing manual human input required, whilst also directly incorprating a human's domain expertise. The Dirichlet Variational Processes used in \citep{friedman11} as a part of their `active learning' is an example of this. By employing a probabilistic model over the image features during the unsupervised clustering process, every label is given a degree of un/certainty. The clustering algorithm can then be required to ask for a manual classification for a particular image if the level of certainty is too low or unsatisfactory which is fixed such that the model can no longer further modify this particular label.

\subsection{Map Creation}
The final step is map creation, where the labelled truthing data is combined with the bathymetry data to generate predictive maps corresponding to the area over which bathymetry data exists. This is the focus of the next section of this literature review, and of this overall study as well. There are two main ways in which acoustic bathymetry data collected can be used for map generation, as described in \citet{ahsan11}. The first approach involves the direct clustering of the acoustic data, then retroactively collecting truthing data in the relevant locations to determine what physical habitats the clusters represented. This is inherently flawed, as it assumes that all the bathymetry information is close for the same habitat and far between habitats. A simple instance of this would be for two areas with sand at significantly different depths (with traces of other habitats too), potentially causing such an approach to identify one as sand, and the other as the trace habitat, if insufficient truthing data is collected. 

The other approach is to first collect and cluster the truthing data before modeling its relationship with the acoustic data, where we apply this relationship to the areas without truthing data to create the habitat map. The latter approach is the one taken in this study, and the basis upon which the following review of techniques used for benthic habitat methods is based upon. An illustrative example of this can be seen in \Cref{fig:bathy_tour_ex} - after the collection of all the bathymetry data points and images, one way for the data to be used it to first assign independently label every image with its correct habitat using a clustering algorithm (unsupervised learning), then assign each image to the closest bathymetry data point. All the data points with assigned images will form the training data, as their `true' labels are known, an the remaining input data will become the test points, whose labels need to be predicted.

% %%%%%%%%%%%%%%% Non-ML Habitat Mapping %%%%%%%%%%%%%%%
% \section{Non-Machine Learning Methods}

While the majority of modern papers in benthic habitat mapping employ machine learning techniques for map creation, this doesn't exclude those that do not from providing useful information and insight. An important study was undertaken in 2001 that employs relatively basic statistical analysis, employing different forms of variance as its main tool of analysis. ~\citep{kostylev01}, at the time (and in fact, even now) integrated more sources of data together than most other studies undertaken - multibeam bathymetric data, geoscientific data, seafloor photographs, habitat complexity, and relative current strength. Rather than using a single model to fit and train data as is more traditionally done in machine learning, multiple statistical tools are used in a peacemeal manner including One-way analysis of variance (ANOVA), Student-Newman-Keuls (SNK) tests, and Analysis of Covariance (ANCOVA). 

Although little is done to address and verify accuracy of the actual results/map in this paper, it provides value through the analysis of variance and covariance performed on and between different benthic/marine properties, establishing relationships typically taken for granted or ignored. For example, it is established that while sediment type contributed heavily to a higher taxonomic\footnote{Taxonomy in a biological context is the categorisation of different organisms based on shared characteristics.} group count, there was little relationship between sediment type and depth. However, this only indicates that there is no linear relationship between the two, and doesn't necessarily preclude a non-linear relationship between them, perhaps with the inclusion of other parameters as well. In particular, Kostylev establishes that gravel subrates are more abundant with varying taxonomic groups than their sand counterparts.

Such findings provide useful insights for future studies that will allow a better assessment of data, or to be able to apply initial assumptions in obtaining better results. However, constantly seeking a deeper understanding through a proportionally increasing amount of sampling creeps towards 'exploring' the entire Earth's oceans manually. To obtain economically feasible yet reliable predictions from the limited data that we have, we need to formally employ machine learning techniques to further utilise the information that we gather.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Evolution of Map Creation Methods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deterministic Machine Learning Algorithms in Benthic Habitat Mapping}
As benthic habitat mapping covers a diverse range of discplines, namely "marine biology, ecology, geology, hydrography, oceanography and geophysics"~\citep{cjbrown11} in addition to statistics and machine learning, it is logical that it would take considerable effort and vast resources to give each relevant discpline an equal, and large amount of attention within any single study. Thus, different studies can rely on the collective findings of others to launch their own research and look further into particular lines of inquiry, start new ones altogether, or evaluate effectiveness of methods used in the field/etc. Within benthic habitat mapping, one prevalent line of inquiry is how to create more accurate, higher quality maps by employing machine learning techniques. For the remainder of this review, we will be looking at common deterministic machine learning techniques and their performance when used to generate habitat maps. A deterministic algorithm gives a single output or outcome given some input, while a probabilistic one will give a possible \textit{distribution} of outputs and a measure of the likelihood of occurrence over this distribution. 

Some of the most commonly used classification algorithms in benthic habitat mapping are also commonly used across domains involving classifying data in general. These include support vector machines (SVMs), random forests, k-Nearest neighbour classifiers, and logistic regression. Although these are more traiditional, more well known methods, they are still used in new research, for example in \citet{wahidin15}, where all of the aforementioned algorithms were tested. Although empirical studies have been performed on these methods to compare their performance across many domains such as in \citet{caruana06}, their effectiveness in benthic habitat mapping varies from study to study, depending on the benthic location that can affect variables such as the sort of habitats present and hence the overall physical properties a particular benthic region, how quickly habitats transition from one to another, or the level of biodiversity/mixing of habitats.

Multinomial logistic regression builds upon linear regression, a staple of machine learning algorithms. Its basic formulation only allows two classes to be predicted at a time - multiple logistic regressors are required to train one class against the rest to generalise beyond only $2$ classes. Though it was ranked towards the bottom of Caruana's average performance rankings, it has had some limited success in less complex binary problems where it was able to distinguish between hard-rock bottoms at the benthos with those that were not with 70\% accuracy \citep{dunn09}. In other instances, however, it was only able to get as little as $23\%$ better than a random guess\citep{belander12}, suggesting its unsuitability with the data that was available.

As the overall $2$-nd ranked machine learning algorithm in Caruana's empircal study across 11 different problems, it is no surprise that Random Forests appears in many studies (~\citet{lucieer13}, ~\citet{seiler12}, ~\citet{hasan14}), providing good results in contrast to logistic regression whilst being low in complexity. However, ~\citet{lucieer13} found that while random forest classifiers were most able to classify substratum and rugosity, K-nearest neighbour classifiers most accurately classified sponge structure classes, pointing to the need to do a more systematic comparison of different methods in benthic habitat mapping. This highlights the fact that even models that are considered state of the art can be suboptimal without necessarily being subject to extreme conditions. A further advantage to using random forests as pointed out in ~\citep{hasan14} is that it can provide insight into which features were more important than others, which can aid future studies to be more successful and efficient by focusing more efforts towards collecting the most influential data. The success met with using random forests make it a good benchmark to compare against for future work that aim to develop methods to create more accurate benthic habitat maps than has been done before.

\section{Probablistic Methods in Benthic Habitat Mapping}
Although the raw performance of deterministic methods like random forests is respectable in most cases, they are not able to quantify uncertainty in their results, and hence, decisions that need to be made using the predictions from these machine learning algorithms are similarly unable to quantify risks that may be associated with actions taken as a result of trusting said predictions. 

To be able to provide such information when performing predictions, probabilistic models are required. Gaussian processes are considered a state of the art method to provide probabilistic predictions. Its output takes the form of probabilistic means and variances, which define an `expected' value for any given input features, whereas the variance ditates how much that expected value can deviate based on the level of uncertainty inherent as a result of only working with observations, vs. the \textit{true} underlying function. They have been shown to provide good performance in benthic habitat mapping (~\citet{bender12}), but are constrained by their $O(n^3)$ complexity, severely limiting the number of points that the model can work with. We explore probabilistic classification in more detail in \Cref{chap:gps}, as well as how to scale it to work with larger datasets.

%%%%%%%%%%%%%%% Probabilistic Methods %%%%%%%%%%%%%%%
% \todo{EVERYTHING FROM HERE LIT REVIEW SHOULD BE REMOVED}
% \subsection{Probabilistic Methods}
% The classifications being made regarding benthic habitats naturally involve uncertainty, as we are still learning the relationship between differnt characteristics of benthos with the varying communities of fauna and flora that reside there. Whilst guessing the most likely class for a particular domain deterministically has its practical applications, it is arguably more \textit{natural} to represent the uncertainty ~\citep{rasmussen06}. As our understanding of marine environments is still quite weak ~\citep{un04}, it is debatable whether deterministic results are always appropriate when being used to make high level management decisions relating to marine environments. While deterministic methods will create a model that attempts to explicitly account for all variables, probabilistic models deal with joint distributions over all the variables. As we need to better understand "the complexities of coastal system functioning rather than simplfying and scaling down the system into smaller components" ~\citep{diaz04}, this feature can be esepcially valuable seeing as there is simply not enough 'expert knowledge' to adequately, explicitly model the relationship across a range of variables.
% 
% \paragraph{Illustrative Example} \todo{not an illustrative example - give some actual figures/graph of crossover point} A simple example of this can be seen when comparing the deterministic approach of a logistic regression classifier, with the probabilistic Naive Bayes classifier. Starting from no data, up until a certain threshold, a Naive Bayes (NB) classifier will actually provide a more accurate classification as it approaches its comparatively higher asymptoptic quicker, after which point, one there is sufficient data, the logistic regressor will provide the better results ~\citep{ng04}. In this simplified example, an analogy can be drawn where the data used up to the threshold when the NB classifier performs better represents a lack of knowledge about the data causing the logistic regressor to underperform, where as the continued addition of data represents more understanding (more data points) of the domain, allowing logistic regression to then outperform the NB classifier.
% 
% \paragraph{Gaussian Mixture Models} Gaussian mixture models (GMMs) are parametric models that "model the distribution of data as a set of clusters, where each cluster is a multivariate Gaussian"~\cite{ahsan11}. In this particlar paper, GMMs are compared with classification trees, which it is found to perform better than in most cases, but were also predicted classes from unseen data with higher certainty than discriminative methods. This is because of its generative nature that accounts for the distribution of bathymetric features, allowing it to model the joint distribution of the classes as well as features. Moreover, each \todo{(function = distribution)}Gaussian function within the model has its own mean and covariance matrix, which also contributes to its powerful modeling ability. However, the use of GMM may have been hindered by the dimensionality of the data - while only five properties were measured, each was calculated for a varying number of scales for the input vector, meaning the 'features' were at least some multiple of five. As this exceeds the recommended six dimensions for use with GMM, application to a very large dataset may be beyond reasonable computational ability.\footnote{Nickgillian.com. (2016). GMM Classifier — NickGillianWiki. [online] Available at: http://www.nickgillian.com/wiki/pmwiki.php/GRT/GMMClassifier}. To avoid this, the feature vector may have to be truncated to contain the bathymetric properties for only one particular scale at a time.
% 
% \paragraph{Using Gaussian Processes} A recent study used probabilistic methods to develop a mapping between the clustered acoustic data to continuous cluster probabilities, as opposed to discrete cluster labels, thus representing the certainty of the results obtained. Using Gaussian Processes which do not inherently support classification, ~\citet{bender12} extended the probabilistic least squares classifier to retain the information regarding certainty of class membership that exists during the classfication process, rather than discarding it in the traditional method. By evaluating the probabilistic results of PTLSC by comparing its results with the actual cluster probabilities obtained in the classification of the images via an unsupervised variational Dirichlet process model, it was shown that the PTLSC method performed better than a PLSC trained directly on the discrete cluster labels in terms of accuracy, mean squared error, and mean variance as well. This demonstrates that while both PTLSC and PLSC err in their predictions when dealing with the transition different boundaries, by maintaining probabilistic information in the PTLSC, it is able to make slightly better judgements in such cases.
% 
% \paragraph{Gaussian Processes and large datasets} However, Gaussian processes involve a matrix inversion process that requires an O($n^3$) operation which does not scale well with large datasets. To overcome this whilst reaping the benefits of Gaussian processes, ~\citet{bender12} extracted subsets of the original dataset on which to perform analysis - a small, randomly chosen portion from three Gaussians, of the initial millions of observations. While this has still provided a high accuracy for all methods tested, there is likely information to be gained by being able to use a considerably larger portion of the dataset. To do this, a method would be required to generate sparse covariance matrices through approximations ~\citep{bickel08}, or use of functions that guarantee sparseness as a property~\citep*{melkumyan09} - something that can be explored in future work. To illustrate how the obstacle can be overcome, the latter paper describes a method whereby, rather than inverting the covariance matrix in its raw form, a threshold is calculated at which point, rather than observing the normal 'tapering' off of covariance values, they are simply set to zero beyond that point. This will result in a significant portion of the covariance matrix being populated with 0s, at which point inversion of the sparse matrix can be performed for which there are known efficient methods. However, there have been more ways of sparse approximation GPs that other studies have explored.
% 
% \paragraph{Sparse Approximation Gaussian Processes}
% 
% \todo{m isn't clarified here, plus n<m is incorrect, should be m<<n} 
% 
% \todo{lots of things here have become irrelevant (talks about 'optimal' methods that aren't implemented/included in experiments - limit to relevant ones, i.e. the GP ensemble methods}
% 
% It is of importance that a number of methods of dealing with sparse approximation of GPs are taken into account if the aim is to deal with large GPs in the inversion step. ~\citep{candela05} explores exactly this, immediately discounting the "subset of data" (SoD) method as being non-competitive due to it not being able to represent the original data to a reasonably accurate enough extent, though we have seen that this was the approach taken in ~\citep{bender12}. As all the different methods (bar SoD) have a complexity of $O(nm^{2})$ where $n$ is the size of the data, and $n<m$, the authors notably point out that no gross approximations should be made as more competent methods are computationally equivalent, and as such point towards their notes on future work to outperform the existing state of the art. As such, we would wish to explore combining the \textbf{Partially Independent Training Conditional approximation} with "the most powerful selection method for the inducing inputs." 
% 

\section{Summary}
In this chapter, we briefly looked at how the data used in benthic habitat mapping is collected, followed by a review of deterministic methods that have been used in existing studies. It is evident that due to the varying and even unknown nature of the intricacies of the benthos, there is not any single stand-out method that can be labelled as the `best' option when choosing how to create predictive habitat maps. Given that the purpose of these maps are to be able to economically map out what large swaths of the ocean look like so that better decisions can be made to conserve and protect them, the need for probabilistic mapping methods become apparent. Such methods are still able to generate labels for input data as we have seen so far, but also provides a level confidence per prediction that allows any action being taken related to a benthic area to have an numerical risk value attached to it. For example, if a large amount of resources are being spent on the conservation of coral in a particular area, decision makers should want to know that a model is $67\%$ confident that a section of benthos is now $62\%$ bleached coral so that they can take extra precautions into account in the event that the the prediction is incorrect, falling into the other $33\%$. Being able to estimate this uncertainty and hence quantify risk prior to taking any actions is essential to benthic management efforts, and is the motivation for methods explored in the next chapter.
