\chapter{Literature Review} \label{chap:litreview}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OVERVIEW %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this chapter, an overview of will be provided of what benthic habitat mapping is and the general steps involved in data collection, followed by a review of some of most commonly used approaches when performing benthic habitat mapping. The practice of benthic habitat mapping precedes the rapid developments in machine learning methods in recent history, and as such, early attempts would naturally have involved manual predictions based on available information that would be subject to the biases of experts involved in the process. It is thus expected that given the same raw data, different experts who have had varying experiences in their field would come to different conclusions. 

This phenomena was observed in a geoscience related study \citep{bond07}, where over 400 individuals with geoscience backgrounds were asked to assess a synthesised seismic image, with just under 25\% correctly identifying the `true' tectonic setting and the three main fault strands. Interestingly however, (inexperienced) students were as likely to give incorrect responses as those with over 15 years' experience, where the latter often drew conclusions linked to the area that the held expertise in. Early efforts to create benthic maps followed this trend, where the lack of more formalised approaches meant experts would use the available data to extrapolate habitat maps based on the understanding that they had. This points to the variability of expert-driven modeling of natural environments such as benthic habitats, and the need for data-driven techniques, where expert input can be used as a supporting source of information rather than the only, or dominant one. 

\section{Benthic Habitat Mapping Overview}

The process of benthic habitat mapping involves three key steps that the large majority of all studies in the area go through\citep{ozcoasts}. In this section, we will give a brief overview of each of these steps, along with common procedures involved.
\begin{itemize}
    \item \textbf{Habitat Characterisation} - extracting properties of the environment such as rugosity (roughness), aspect (direction of slope), depth
    \item \textbf{Habitat Classification} - grouping the raw information about the environment into categories, such as sand, granite, etc.
    \item \textbf{Habitat Mapping} - using classifications with the larger scale bathymetry data to extrapolate habitat maps 
\end{itemize}

%%%%%%%%%%%%%%% Habitat Characterisation %%%%%%%%%%%%%%%
\subsection{Habitat Characterisation}
If high resolution, multimodal\footnote{Multimodal data refers to the different \textit{information} that resides within it - e.g. an area of benthic terrain can be represented by a single photograph, or numerical representations of its physical properties.} data for the entire ocean's benthos was easily obtainable, creating benthic habitats maps for any given area would be not be an incredibly difficult task. As this is prohibitively expensive, the altnerative is instead collecting relatively large amounts of economically obtainable information such as bathymatery data, and comparatively fewer samples of data that are costly to collect such as images (so that a relationship can be modeled between them, to be explained below). This subsection provides a brief summary of data collected and methods used to do so.

\paragraph{Remote-sensing data}
As exhaustively exploring Earth's ocean floor with underwater vehicles to capture it visually as well as all its physical properties is an infeasible task, compromise is required so that modest amounts of data can be collected economically, whilst still being informative. Remote-sensing data is thus used, usually obtained from acoustic backscatter methods, which involes the firing of sound waves towards the benthos, where their frequency and strength upon returning is used to deduce the physical properties of the area from where it rebounded, such as depth, slope, and roughness. More modern acoustic backscatter collection methods like Multibeam echosounders (MBES) are gradually superceding older ones such as Single-beam echosounders (SBES)\citep{calvert15} due to the \textit{cone} of the beam angle of the sound pulse in MBES being of a considerably smaller angle to reduce the amount of coarse, repeated data needed to try and resolve the true properties contained. \citep{cjbrown11}. At the same time, it maintains a larger swath angle, allowing data to be collected at a fast rate.

\begin{figure}[H]
    \includegraphics[scale=0.8]{sbes_mbes.png}
    \caption{Rough comparison of SBES with MBES \citep{braveheartmarine}}
    \label{fig:sbes_mbes}
\end{figure}

As pointed out in \cite{calvert15}, bathymetry data alone is not enough and should not be the sole basis on which habitat maps are formed, and as such, more conclusive information is required to make predictions in these studies.

\paragraph{Truthing Data}
One of the more common methods to be able to obtain a sufficiently large truthing data set (but small relative to areas with remote-sensing data) are videos or images, generally by autonomous means, where images have the advantage of not requiring the additional post-processing that video does to extract the images. Technology to send unmanned vehicles to the benthos and capture visual data is a relatively recent development, and prior to that, it was more common to send divers on manual expeditions that involved actually collecting sediment samples that were used to identify what the habitat at a particular location may have been. 

% The advantage that be provided , however, is the redundancy in data points ~\citep*{rattray14}  - but there is extra cost in time required to convert videos into the needed images (pre-proessing before feeding into algorithms for habitat mapping), an area that is in itself worth of research within the field.~\citep*{lucieer13}
% 
% \paragraph{Other data}
% \todo{(why is water column correction important when correlating images with seagrass standing crop?)} Other data that is less common, but also used to map habitats, is patterns in the water movement (such as tidal currents, wave action)~\citep*{cjbrown11} in the column of water above the area of benthos being mapped - a feature that provided useful input in arriving at an accurate benthic habitat map (in addition to sediment analysis).~\citep*{snelgrove94} Other sources such as UNESCO have also verified the importance and significance of using water column correction techniques to obtain more accurate habitat maps, particularly when correlating images with segrass standing crop. \footnote{Unesco.org. (2016). Water column correction techniques. [online] Available at: http://www.unesco.org/csi/pub/source/rs10.htm} 

%%%%%%%%%%%%%%% Habitat Classification %%%%%%%%%%%%%%%
\subsection{Habitat Classification}

\todo{more in-depth focus here, what kind of supervised/unsupervised ML algorithms are used for classification?} Almost all studies use \textit{in situ}\footnote{in situ, in a biological context, refers to the precise spot in which something occurs. In a habitat mapping context specifically when referring to truthing data, it simply means, in the case of bathymetry and image data, images taken at the exact spot corresponding to a particular bathymetry sampling location.} 'truthing' data to complement the acoustic data to be able to build a model between the acoustic data and truthing data (creation of these models are explained in following sections). However, we need to know the labels of this data considering that the final goal is to create a habitat map, where any one habitual zone is given its prospetive label - to do this, we also need to label the clusters of truthing data. These categories may be, for example, `bedrock covered by discontinuous seagrass cover', `Maerl interspersed with sand and gravel', `superficially coarse sand to fine gravel covered by dense patches of seagrass', etc.~\citep{micallef12}. Given physical sediment truthing data, the process of labeling locations is inevitably a manual one, but this is not the case when dealing with image data. The two overarching ways to perform this classification with images are using supervised and unsupervised algorithms.

Studies have used both unsupervised clustering and supervised clustering (classification) to label the truthing data for the model-fitting stage. Often, there may be large amounts of visual data, beyond that which any human or even team can reasonably, manually cluster - and as such, unsupervised algorithms are first used to create these clusters, after which an expert may be brought in to verify/simplify (or otherwise) the resulting clusters. One possible method in the unsupervised category is to use hierarchical clustering as seen in \citet{pizarro09}. This is an approach in which a layered tree\footnote{the tree described here is formally known as a \textit{dendrogram}} is formed where the two nearest clusters (based on a pre-deinfed distance metric) are grouped together to form a larger cluster, with a base case where every point is its own cluster. The distance metric used in this particular study was the Kullback-Leibler convergence between points, where each was attributed to a distribution of a set of features, including properties such as saliency that was calculated using colour and texture of the image, as well as colour histograms of comprehensively normalised images.

\begin{figure}[H]
    \includegraphics[scale=0.6]{pizarro-dendrogram.jpg}
    \caption{Dendrogram of images from  \citet{pizarro09} using Kullback-Leibler convergence distances between feature distributions. Clusters in the lower layers were ommitted to prevent visual clutter.}
    \label{fig:p-dendrogram}
\end{figure}

Through more complex approaches, the supervised and unsupervised paradigms can be combined to get benefit from the advantages of both - reducing manual human input required, whilst also directly incorprating a human's domain expertise. The Dirichlet Variational Processes used in \citep{friedman11} as a part of their `active learning' is an example of this. By employing a probabilistic model over the image features during the unsupervised clustering process, every label is given a degree of un/certainty. The clustering algorithm can then be required to ask for a manual classification for a particular image if the level of certainty is too low or unsatisfactory which is fixed such that the model can no longer further modify this particular label.

\subsection{Map Creation}
The final step is map creation, where the labelled truthing data is combined with the bathymetry data to generate predictive maps corresponding to the area over which bathymetry data exists. This is the focus of the next section of this literature review, and of this overall study as well. There are two main ways in which acoustic bathymetry data collected can be used for map generation, as described in \citet{ahsan11}. The first approach involves the direct clustering of the acoustic data, then retroactively collecting truthing data in the relevant locations to determine what physical habitats the clusters represented. This is inherently flawed, as it assumes that all the bathymetry information is close for the same habitat and far between habitats. A simple instance of this would be for two areas with sand at significantly different depths (with traces of other habitats too), potentially causing such an approach to identify one as sand, and the other as the trace habitat, if insufficient truthing data is collected. 

The other approach is to first collect and cluster the truthing data before modeling its relationship with the acoustic data, where we apply this relationship to the areas without truthing data to create the habitat map. The latter approach is the one taken in this study, and the basis upon which the following review of techniques used for benthic habitat methods is based upon.

%%%%%%%%%%%%%%% Non-ML Habitat Mapping %%%%%%%%%%%%%%%
\section{Non-Machine Learning Methods}
\todo{this section should be part of the previous one (Map Creation)} While the majority of modern papers in benthic habitat mapping employ machine learning techniques for map creation, this doesn't exclude those that do not from providing useful information and insight. An important study was undertaken in 2001 that employs relatively basic statistical analysis, employing different forms of variance as its main tool of analysis. ~\citep{kostylev01}, at the time (and in fact, even now) integrated more sources of data together than most other studies undertaken - multibeam bathymetric data, geoscientific data, seafloor photographs, habitat complexity, and relative current strength. Rather than using a single model to fit and train data as is more traditionally done in machine learning, multiple statistical tools are used in a peacemeal manner including One-way analysis of variance (ANOVA), Student-Newman-Keuls (SNK) tests, and Analysis of Covariance (ANCOVA). 

Although little is done to address and verify accuracy of the actual results/map in this paper, it provides value through the analysis of variance and covariance performed on and between different benthic/marine properties, establishing relationships typically taken for granted or ignored. For example, it is established that while sediment type contributed heavily to a higher taxonomic\footnote{Taxonomy in a biological context is the categorisation of different organisms based on shared characteristics.} group count, there was little relationship between sediment type and depth. However, this only indicates that there is no linear relationship between the two, and doesn't necessarily preclude a non-linear relationship between them, perhaps with the inclusion of other parameters as well. In particular, Kostylev establishes that gravel subrates are more abundant with varying taxonomic groups than their sand counterparts.

% Certain organisations, government bodies/etc. will also provide guidelines outlining the classification proess. For example, the European Nature Information System website and the Australian Government's 'Interim Marine and Coastal Regionalisation for Australia'\footnote{Unesco.org. (2016). Water column correction techniques. [online] Available at: http://www.unesco.org/csi/pub/source/rs10.htm} both provide classification schemes for people creating habitat maps or other similar efforts.

Such findings provide useful insights for future studies that will allow a better assessment of data, or to be able to apply initial assumptions in obtaining better results. However, constantly seeking a deeper understanding through a proportionally increasing amount of sampling creeps towards 'exploring' the entire Earth's oceans manually. To obtain economically feasible yet reliable predictions from the limited data that we have, we need to employ machine learning techniques to further utilise the information that we gather.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Evolution of Map Creation Methods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Deterministic Machine Learning Algorithms in Benthic Habitat Mapping}
As benthic habitat mapping covers a diverse range of discplines, namely "marine biology, ecology, geology, hydrography, oceanography and geophysics"~\citep{cjbrown11} in addition to statistics and machine learning, it is logical that it would take considerable effort and vast resources to give each relevant discpline an equal, and large amount of attention within any single study. Thus, different studies can rely on the collective findings of others to launch their own research and look further into particular lines of inquiry, start new ones altogether, or evaluate effectiveness of methods used in the field/etc. Within benthic habitat mapping, one prevalent line of inquiry is how to create more accurate, higher quality maps by employing machine learning techniques. For the remainder of this review, we will be looking at common deterministic machine learning techniques and their performance when used to generate habitat maps. A deterministic algorithm gives a single output or outcome given some input, while a probabilistic one will give a possible \textit{distribution} of outputs and a measure of the likelihood of occurrence over this distribution. Some mathematical background will be given where relevant in the following algorithms, as their application will be revisited in \autoref{chap:experiments}.

\subsection{Multiple Multinomial Logistic Regression}
The underlying function of logistic regression is that used in linear regression, so a brief overview of linear regression will first be given. The motivation for linear regression is that given some input data $\mathbf{X}$, corresponding response values (or output) $\mathbf{y}$, and a set of weights $\mathbf{\beta}$ are defined to best define the relationship between the two variables as below. Note that the intercept term has not been included in this basic explanation, as it can be included by augmenting the inputs $\mathbf{X}$ with a bias term $1$ which would have its own corresponding weight $\beta_i$ as well.

\begin{equation}
\begin{split}
    \mathbf{y} = \mathbf{\beta X} \\
    y_i = \beta_i x_i,         i = 1,...,n
\end{split}
\end{equation}

To find these set of weights, we need to optimise this equation with respect to weights $\mathbf{\beta}$ such that $\mathbf{\beta x}$ is as close to $\mathbf{y}$ as possible. In other words, we wish to minimise the errors, or residuals. Once they have been found, the weights are then applied to any future inputs with unknown response values \todo{(extra details on optimisation here)}. The restriction of this only finding linear relationships can be lifted by projecting inputs into higher dimensions, which allows `linear' regression to then model more complex data.

To then perform multiple multinomial logistic regression, we need to first manipulate the multi-class label data into binary labels, such that for some set of $n$ discrete labels $1, 2,...$, we consider $n$ cases where for the $i-$th case, $i$ takes on the value $1$, whilst all others becomes $0$ (see Example \autoref{ex:one-vs-all} for an example). This is needed as logistic regression only allows predictions of binary labels - this corresponds to a one-vs-all logistic regressor. However, this doesn't guarantee that predictions that each underlying linear regressor will fall within the $[0, 1]$ boundary. To ensure this, we use the logistic function (\autoref{fig:logistic}) to `clamp' all response variables back between $0$ and $1$, where $\alpha$ is a single prediction, or a vector of them:

\begin{ex} \label{ex:one-vs-all}
    $$1, 1, 1, 2, 2, 2, 3, 3, 3$$
    $$1\text{-vs-all: }1, 1, 1, 0, 0, 0, 0, 0, 0$$ 
    $$2\text{-vs-all: }0, 0, 0, 1, 1, 1, 0, 0, 0$$ 
    $$3\text{-vs-all: }0, 0, 0, 0, 0, 0, 1, 1, 1$$
\end{ex}

\begin{equation} \label{eq:logistic}
    \logistic(p) = \frac{1}{1+exp(-\alpha)}
\end{equation}

\begin{figure}
    \includegraphics[scale=0.6]{sigmoid.png}
    \caption{Logistic Function}
    \label{fig:logistic}
\end{figure}

This will then provide a vector of predictions with the same length as the test inputs for \textit{each} label, representing a probability that that particular data point is of that label. To simplify this into a set of output labels as desired, one would simply need to take the argmax of predictions per label. For example, given these raw predictions for a dataset with 4 possible output labels, $1, 2, 3, 4$:

\[
\begin{bmatrix}
    0.005 &  0.370 &  0.314 &  0.150 &  0.559 &  0.512\\
    0.492 &  0.236 &  0.911 &  0.540 &  0.539 &  0.383\\
    0.536 &  0.435 &  0.647 &  0.154 &  0.511 &  0.737\\
    0.684 &  0.773 &  0.670 &  0.667 &  0.990 &  0.944\\
\end{bmatrix}
\]

The max values per column are: $0.559 0.911 0.737 0.990$, corresponding to the labels $3, 3, 1, 3, 3, 3$.

Multiple Logistical Regression is one of the more basic machine learning algorithms that can be used to predict habitat classes. Regression, broadly, involves the estimation of relationships between variables, and logistic regression involves the prediction of likelihood of class membership given a number of variables (that are assumed to have low collinearity\footnote{Collinearity between two variables refers to how correlated they are - a contrived example would be measurements of depth taken in centimetres and metres.}). This only applies to domains with two classes, however - to use this technique for classification where we have an unbounded  number of classes (though usually still relatively low), we need to use multinomial logistic regression, which is able to account for more than two distinct, unordered (i.e., sand vs. mud has no relative ordering) classes, where class membership is predicted using maximum likelihood estimation (MLE), similarly to logistic regression. However, the difference is that whereas logistic regression only requiring a single logit function as the variable being predicted is a binary one, multinomial logistic regression requires comparison between $k-1$ (where $k$ is the number of possible dependent variables) logit functions. 

Even though ~\citet{caruana06} show that logistic regression methods achieve on average worse results than most other approaches available, it recognises that in certain cases the models that perform most poorly on average still display exceptional performance, and as such, this method is still worth exploration and experimentation. In particular, ~\citet{belanger12} used multinomial logistic regression across temperature, salinity, and productivity to correctly predict class membership by a margin of 23-84\% more than by pure chance. This is equivalent to an improvement of 1-2x compared to a random guess, which taken at face value would suggest that logistic regression is an undesirable choice of algorithm for this problem domain.

\subsection{Random Forests}
In contrast to logistic regression, random forests were shown in ~\citet{caruana06} to be state of the art, only just falling short of boosted decision trees after callibration. Random forests are an ensemble method, meaning that it uses a collection of estimators, before aggregating their results to obtain some sort of average. The aim of this is to minimise the variance and hence error that any single one of these estimators would otherwise result in. 

From the initial dataset, some number $B$ is chosen which represents the \textit{number} of trees to build (as a part of our random forest), after which, $B$ random, unique subsamples of the full dataset are taken. Within each decision tree in our random forest, some constant number $m$ of features is taken at each node of the tree, such that the split at each node only takes into account the $m$ randomly chosen features. Each of the decision trees in our forest will hence have a 'result' (that may be a class or some continuous value). Typically, the final decision of the random forest will be made by a vote count for classification, and an average of each decision tree's result in regression problems.

As random forests are a method that is low in complexity but provides very good results on average, we can see that it is used in quite a few studies (~\citet{lucieer13}, ~\citet{seiler12}, ~\citet{hasan14}), where the random forest classifier provided the best results over other methods relating to at least a significant subset of the explored data. However, ~\citet{lucieer13} found that while random forest classifiers were most able to classify substratum and rugosity, K-nearest neighbour classifiers most accurately classified sponge structure classes, pointing to the need to do a more systematic comparison of different methods in benthic habitat mapping. A further advantage to using random forests as pointed out in ~\citep{hasan14} is that it can provide insight into which features were more important than others, which can aid future studies to be more successful and efficient by focusing more efforts towards collecting the most influential data. The success met with using random forests make it a good benchmark to compare against for future work that aim to develop methods to create more accurate benthic habitat maps than has been done before.

\subsection{Multi-class Support Vector Machines}

Multi-class support vector machines require similar additional steps to those required by logistic regression to be able to do multiple (multi-class) multiple logistic regression. SVMs also only inherently support two classes when used for classification.

\begin{figure}
    \includegraphics[scale=0.3]{svm.png}
    \caption{Basic example of an SVM}
    \label{fig:svm}
\end{figure}

In \autoref{fig:svm}, we see an SVM find a plane (or line, in the 2D case) that maximises the distance to the closest point of each of the other two classes. To generalise this to any number of classes greater than $2$, one option is to consider a one-vs-one approach in contrast to the one-vs-all one in Example \autoref{ex:one-vs-all}. In this case, an SVM is build for all pairs of labels, meaning that for $c$ unique labels, $\frac{c(c-1)}{2}$ separate SVMs are needed. When performing predictions, a tally is kept for each input data row counting how many of each class they were predicted to be by all the SVMs, with the highest count being the final prediction.

Although support vector machines fell outside the top three overall supervised learning algorithms in terms of accuracy in \citet{caruana06}'s work, their non-parametricity could potentially be of benefit given that our knowledge of the complex relationships between elements of benthic habitats are limited. Moreover, despite SVMs not being used often in the field, they are "acknowledged to be very competitive descriminative classifiers in machine learning literature"~\citep{ahsan11}. 

However, SVMs in their base form only support classification into two classes, requiring modification to the original algorithm to support more - an active area of research that has not found a `best' way to perform this algorithm extension yet. To do so, there are two, basic main approaches available, one being a \textbf{one vs. all} approach, and the other being a combination of all \textbf{one vs. one} approach. We can hence quite clearly see that given $C$ possible classes, the first would require $C$ separate classifiers, whereas the latter would require $\frac{C(C-1)}{2}$ classifiers~\citep{murphy12}. Multi-class SVMs were used in ~\citet{ahsan11} for illustrative purposes, using the one vs. one approach as per their use of LibSVM~\citep{chang11}, outperforming classification trees on certain datasets, and in limited cases (but not overall), Gaussian Mixture Models as well. Again, the mixed results would suggest that under certain conditions (such as size of the dataset and various properties of the data itself, some of which are not known before testing), use of a multi-class SVM could provide a useful benchmark to some extent.

\section{Summary}
In this chapter, we briefly looked at how the data used in benthic habitat mapping is collected, followed by a review of deterministic methods that have been used in existing studies. It is evident that due to the varying and even unknown nature of the intricacies of the benthos, there isn't any single stand-out method that can be labelled as the `best' option when choosing how to create predictive habitat maps. Given that the purpose of these maps are to be able to economically map out what large swaths of the ocean look like so that better decisions can be made to conserve and protect them, the need for probabilistic mapping methods become apparent. Such methods are still able to generate labels for input data as we have seen so far, but also provides a level confidence per prediction, meaning that a statement such as `this area of benthos is 75\% sand and 25\% coral', for example, could instead be more informative - `this area is on average 83\% likely to be 75\% coral, and 74\% to be 25\% coral' (with the remaining probable distributions excluded for simplicity). These techniques are the focus of the next chapter.

%%%%%%%%%%%%%%% Probabilistic Methods %%%%%%%%%%%%%%%
% \todo{EVERYTHING FROM HERE LIT REVIEW SHOULD BE REMOVED}
% \subsection{Probabilistic Methods}
% The classifications being made regarding benthic habitats naturally involve uncertainty, as we are still learning the relationship between differnt characteristics of benthos with the varying communities of fauna and flora that reside there. Whilst guessing the most likely class for a particular domain deterministically has its practical applications, it is arguably more \textit{natural} to represent the uncertainty ~\citep{rasmussen06}. As our understanding of marine environments is still quite weak ~\citep{un04}, it is debatable whether deterministic results are always appropriate when being used to make high level management decisions relating to marine environments. While deterministic methods will create a model that attempts to explicitly account for all variables, probabilistic models deal with joint distributions over all the variables. As we need to better understand "the complexities of coastal system functioning rather than simplfying and scaling down the system into smaller components" ~\citep{diaz04}, this feature can be esepcially valuable seeing as there is simply not enough 'expert knowledge' to adequately, explicitly model the relationship across a range of variables.
% 
% \paragraph{Illustrative Example} \todo{not an illustrative example - give some actual figures/graph of crossover point} A simple example of this can be seen when comparing the deterministic approach of a logistic regression classifier, with the probabilistic Naive Bayes classifier. Starting from no data, up until a certain threshold, a Naive Bayes (NB) classifier will actually provide a more accurate classification as it approaches its comparatively higher asymptoptic quicker, after which point, one there is sufficient data, the logistic regressor will provide the better results ~\citep{ng04}. In this simplified example, an analogy can be drawn where the data used up to the threshold when the NB classifier performs better represents a lack of knowledge about the data causing the logistic regressor to underperform, where as the continued addition of data represents more understanding (more data points) of the domain, allowing logistic regression to then outperform the NB classifier.
% 
% \paragraph{Gaussian Mixture Models} Gaussian mixture models (GMMs) are parametric models that "model the distribution of data as a set of clusters, where each cluster is a multivariate Gaussian"~\cite{ahsan11}. In this particlar paper, GMMs are compared with classification trees, which it is found to perform better than in most cases, but were also predicted classes from unseen data with higher certainty than discriminative methods. This is because of its generative nature that accounts for the distribution of bathymetric features, allowing it to model the joint distribution of the classes as well as features. Moreover, each \todo{(function = distribution)}Gaussian function within the model has its own mean and covariance matrix, which also contributes to its powerful modeling ability. However, the use of GMM may have been hindered by the dimensionality of the data - while only five properties were measured, each was calculated for a varying number of scales for the input vector, meaning the 'features' were at least some multiple of five. As this exceeds the recommended six dimensions for use with GMM, application to a very large dataset may be beyond reasonable computational ability.\footnote{Nickgillian.com. (2016). GMM Classifier — NickGillianWiki. [online] Available at: http://www.nickgillian.com/wiki/pmwiki.php/GRT/GMMClassifier}. To avoid this, the feature vector may have to be truncated to contain the bathymetric properties for only one particular scale at a time.
% 
% \paragraph{Using Gaussian Processes} A recent study used probabilistic methods to develop a mapping between the clustered acoustic data to continuous cluster probabilities, as opposed to discrete cluster labels, thus representing the certainty of the results obtained. Using Gaussian Processes which do not inherently support classification, ~\citet{bender12} extended the probabilistic least squares classifier to retain the information regarding certainty of class membership that exists during the classfication process, rather than discarding it in the traditional method. By evaluating the probabilistic results of PTLSC by comparing its results with the actual cluster probabilities obtained in the classification of the images via an unsupervised variational Dirichlet process model, it was shown that the PTLSC method performed better than a PLSC trained directly on the discrete cluster labels in terms of accuracy, mean squared error, and mean variance as well. This demonstrates that while both PTLSC and PLSC err in their predictions when dealing with the transition different boundaries, by maintaining probabilistic information in the PTLSC, it is able to make slightly better judgements in such cases.
% 
% \paragraph{Gaussian Processes and large datasets} However, Gaussian processes involve a matrix inversion process that requires an O($n^3$) operation which does not scale well with large datasets. To overcome this whilst reaping the benefits of Gaussian processes, ~\citet{bender12} extracted subsets of the original dataset on which to perform analysis - a small, randomly chosen portion from three Gaussians, of the initial millions of observations. While this has still provided a high accuracy for all methods tested, there is likely information to be gained by being able to use a considerably larger portion of the dataset. To do this, a method would be required to generate sparse covariance matrices through approximations ~\citep{bickel08}, or use of functions that guarantee sparseness as a property~\citep*{melkumyan09} - something that can be explored in future work. To illustrate how the obstacle can be overcome, the latter paper describes a method whereby, rather than inverting the covariance matrix in its raw form, a threshold is calculated at which point, rather than observing the normal 'tapering' off of covariance values, they are simply set to zero beyond that point. This will result in a significant portion of the covariance matrix being populated with 0s, at which point inversion of the sparse matrix can be performed for which there are known efficient methods. However, there have been more ways of sparse approximation GPs that other studies have explored.
% 
% \paragraph{Sparse Approximation Gaussian Processes}
% 
% \todo{m isn't clarified here, plus n<m is incorrect, should be m<<n} 
% 
% \todo{lots of things here have become irrelevant (talks about 'optimal' methods that aren't implemented/included in experiments - limit to relevant ones, i.e. the GP ensemble methods}
% 
% It is of importance that a number of methods of dealing with sparse approximation of GPs are taken into account if the aim is to deal with large GPs in the inversion step. ~\citep{candela05} explores exactly this, immediately discounting the "subset of data" (SoD) method as being non-competitive due to it not being able to represent the original data to a reasonably accurate enough extent, though we have seen that this was the approach taken in ~\citep{bender12}. As all the different methods (bar SoD) have a complexity of $O(nm^{2})$ where $n$ is the size of the data, and $n<m$, the authors notably point out that no gross approximations should be made as more competent methods are computationally equivalent, and as such point towards their notes on future work to outperform the existing state of the art. As such, we would wish to explore combining the \textbf{Partially Independent Training Conditional approximation} with "the most powerful selection method for the inducing inputs." 
% 
