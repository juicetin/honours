\chapter{Conclusion} \label{chap:conclusion}

Benthic habitat mapping is a relatively old concept, dating back at at least several decades when photos and videos became a viable method of capturing information about the benthos ~\citep{gibson07} as an alternative to earlier destructive methods of sediment sampling. However, the different sources of data required and the machine learning techniques needed to model the data to be able to predict properties about the benthos did not become more readily available until relatively recently. With tools such as multibeam echosounders for collecting acoustic backscatter data at scale, as well as extensive visual imaging of the benthos via autonomous underwater vehicles, it has become possible to collect large amounts of data about Earth's ocean to work with to gain a deeper understanding. Many studies to date have used machine learning algorithms such as SVMs and random forests on different types of data in different habitats and shown moderately good results, while more state of the art methods employ probabilistic methods such as Gaussian processes~\citep{bender12} that capture a richer set of information in the form of both predictions over possible labels as well as certainties around these predictions, or Gaussian mixture models~\citep{ahsan11} that are able to identify clusters that are individually multivariate Gaussians.

In this study, application of products of experts models was applied to Gaussian processes to lift the usual limitation on data sizes of several thousand points due to the time that would be needed to fit them. In fact, due to the requirement of multiple Gaussians processes to form GP classifiers, this several thousand is effectively scaled down by a factor proportional to the number of classes in the data. Experiments showed that for aggregated labels, the ensembles of experts were comparable in performance to the full GP, though the performance gap grew when dealing with the 24-label case, but keeping in mind that the GP became unusable on the full query data for predictions due to the time required. The predictions on the simple labels of the approximations also matched that of random forests and the distributions of the Diriclet multinomial's predictions.

As all the studies performed in the area of benthic habitat mapping deal with single-label outputs, they would be unable to account for and fully utilise multi-label count data, and resort to approximations by taking the most frequently occuring label per output. To assess the viability of working with multi-output data in benthic habitat mapping, the Dirichlet multinomial was used, serving exactly this purpose. 

\section{Future Work}

\begin{itemize}
    \item perform similar experiments on incrementally changing data every few years - observe biodiversity/habitat changes
    \item replace the simple activation function in the dirichlet multinomial with a more complex model like a GP
    \item previous work has been done for finding least certain areas of a GP to decide where to send AUV's to maximise resulting confidence in habitat labels - use entropy to be able to do the same with dirichlet multinomials, whilst overcoming the problem of areas with consistent heterogenous labels that otherwise confuse GPs
    \item combine habitat data with actual fauna distributions as well
    \item explore other multinomial prior distributions other than Dirichlet, such as Logit-normal distributions?
\end{itemize}

There a number of areas that would be pertinent to explore as an extension of this study, to further the usefulness of the data provided in terms of both the complexities of the underlying models used to predict data, as well as the contexts in which they are used.


