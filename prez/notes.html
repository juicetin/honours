<h1 id="motivationobjective-of-own-work">Motivation/Objective of own work</h1>
<ul>
<li>1 minute</li>
<li>Benthic habitat mapping studies generally use deterministic, single-output methods</li>
<li>For people involved in manging or conserving ocean habitats, they need to be able to quantatively assign risk to any action they may need to take
<ul>
<li>this allows appropriate resources dedicated to backup actions/fallbacks that may need to be put into place</li>
<li>an all-or nothing approach is not an option in terms of any sort of environmental conversation</li>
</ul></li>
<li>probabilistic methods (GPs) have a time constraint</li>
<li>need for multi-output</li>
<li>because of the way data # Background literature</li>
<li>1 minute</li>
<li>go through some basic deterministic approaches used in the past - SVMs, RFs, even LR</li>
<li>mention works that have touched on use of GPs - Bender's 2012 work # Outlining of own approach/method/algorithm</li>
<li>3 minutes</li>
<li>applying GP approximations to mapping to see how they perform relative to deterministic methods</li>
<li>multi-output by not simplifying the data # Main results of own project</li>
<li>2 minutes # Discussion/Evaluation/Contribution of own work</li>
<li>2 minutes # Conclusion and Future Work</li>
<li>2 minutes</li>
</ul>
<h1 id="motivation">Motivation</h1>
<p>Before we delve into it, I'll give a brief primer on what the focus of this study, benthic habitat mapping is. The benthic zone refers to lowest layer of a body of water, such as lake bed or the ocean floor. So, looking at this image, we can get an idea of what benthic habitat mapping is using a simple example. First, bathymetry data is collected over an extensive area so that we know the topography of the benthic zone in question, allowing us to infer other information such as roughness and slope - these are indicated by the blue and orange dots. For a subset of the area, we then collect data that allows us to verify the true habitats - in this case, the dotted black line representing an autonomous underwater vehicle taking images at each black square. By modelling a relationship between the bathymetry data and the images collected where both exists, we can then apply it to the bathymetry data that does not have corresponding labels, to predict the habitats in those areas. The reason for not simply collecting the image data for the entire area is that it is prohibitively expensive, and it's easy to imagine why it's not economically feasible to do this for the entire ocean. By creating these maps, different agencies and government bodies can be better informed and have a better understanding of the state of our oceans, and be able to take appropriate actions to manage and conserve them based on environmental changes.</p>
<p>Here, you can see what a data collection expedition may look like - for each bathymetry data point, several labels are attributed to it as a result of multiple images being taken in that region. The majority of studies use deterministic, single-output methods that require simplifying each bathymetry data point to a single label, as indicated by the coloured circles that take the most frequently occurring label within it.</p>
<p>However, this dilutes the information in a way that results in data loss - looking at the actual counts of different habitat labels in the range of each bathymetry point, there is actually an underlying distribution at each point that is not accounted for.</p>
<p>As a result, methods that simplify the information in this way don't quite capture the full picture, and this study aims to evaluate methods not explored in existing literature to benthic habitat mapping. The first approach is to maintain the of simplification of labels, but to instead provide probabilistic output so instead of dealing with absolutes, saying a particular area is simply 'definitely sand', a probability range is given for every possible label. The second is to directly work with the distributions of labels at each point, performing multi-output predictions that do not simplify the labels counts per habitat in the original data down to only a single label.</p>
<p>The reason we want to do this is to be able to quantify the risk associated with any action that could be taken as a result of trusting the data presented by predictive habitat maps - and to also not discard any underlying information that can be used in the rich data source.</p>
<h1 id="related-literature">Related Literature</h1>
<p>The large portion of related literature in benthic habitat mapping use the same few deterministic methods combining bathymetry data and image data - for example, random forests and support vector machines. Based on the cross validation results, both appear to generally perform quite well, for example in Wahidin's work in 2015 where both are used, but the predictive maps over the area studied aren't shown, so it's uncertain whether they are realistic or not despite cross-validation scores - something that we'll see in the experiments run in this study.</p>
<p>There have been some works in the past several years as well that have started to look at probabilistic benthic habitat mapping - in 2012, Bender, Williams, and Pizarro used subsamples of data and applied Gaussian process classification that allowed them to determine the variance of their predictions. By comparing the results of using label probabilities vs absolute labels, they showed that using the probabilistic information resulted in a lower error and variance than when labels were simplified to a single definitive value. The reason that only a subsample of the data was used was dueo the Gaussian process' O(n^3) complexity from the need for matrix inversions, and using all the training data would have resulted in impractically long training times, particularly when performing 10-fold cross validation.</p>
<h1 id="approach">Approach</h1>
<p>The first area that we explore is how to make full use of the datasets when using probabilistic methods - and not be constrained by Gaussian process' practical limit of several thousand data points due to the matrix inversions. To do this, we consider two related methods of approximation that allow the breaking down of data into smaller problems - namely, product of experts and generalised product of experts. What these methods do is they train separate Gaussian processes on small subsets of the data, for example, lots of 200, and combine all their results, weighting them by their variance, where the lower variance experts are given a higher weighting, and the higher variance ones a lower weighting. This study uses the product of experts and generalised product of experts approximation methods - the difference between them being that while the product of experts uses the variances for weighting directly. The problem with this is that the precisions can add up and cancel each other out - the generalised version aims to reduce this problem by dividing the weight-values by the number of experts. This effect can be partially seen below - for the product of experts, areas between points without data are overconfident, almost having no variance at all - whereas for the generalised version , the same areas still state have a variance interval.</p>
<p>The other method that was investigated is Dirichlet multinomial regression, which allowed us to directly model the label distributions that were present in the original data. TODO - brief explanation of dirichlet multinomial - briefly explain multinomial, then dirichlet and how it is s a distribution of weights of the multinomial distribution</p>
<h1 id="results">Results</h1>
<h2 id="gp-vs-deterministic">gp vs deterministic</h2>
<p>Looking at a comparison of deterministic results with Gaussian processes on this dataset, we can see that the standard GP is able to outperform the most common classification methods used, including those often seen in benthic habitat mapping studies. The approximations don't fare as well, but as we will see, this isn't necessarily indicative of real world performance.</p>
<h2 id="det-maps">det maps</h2>
<p>The maps generated on the full query dataset by fitting the training data are quite different - only the logistic regressor displays notable areas of label 0, whereas random forests display the most diversity throughout the region, with k-nearest neighbours failing to detect anything other than label 3, which was also the most dominant label in the training data.</p>
<h2 id="gp-maps-and-gpogpe">gp maps (and gpogpe)</h2>
<p>The full predictive maps for the standard GP and generalised product of GP experts were quite different, but as we see later, the approximation method's maps are in fact closer to the Dirichlet multinomial's that properly model the original data without simplfying the original labels counts to a single label.</p>
<h2 id="gp---probabilities-and-variance">gp - probabilities and variance</h2>
<p>Looking at the underlying probabilities of the most likely labels at each point, we can see that they all sit in the 62-69% range, which is not very high - with the deviation of each one being quite large as well. This points to the fact that the GP is not very certain about its predictions, and this could likely be due to the biodiversity present in Scott Reef - something we will look at in the next section, when using Dirichlet multinomial regression</p>
<h2 id="dm-maps">DM maps</h2>
<p>Using Dirichlet multinomial regression, we can see how often labels occur and their densities separately - label 3 being the most common, occurring at a rate of more than 50% for over 75% of the points throughout Scott Reef. We can see that label 1 dominates the reef for small section on the far left, with a mix of labels 0, 1, and 3 in the upper-left region.</p>
<h2 id="dm-entropy">DM entropy</h2>
<p>Using the DM, we can also visualise how 'likely' the predictions are using the entropy of each point across the label distribution of every label. This entropy heatmap shows that the entropy for the predictions are quite low throughout the Scott Reef, with no noticable are of high entropy. The particularly dark purple areas indicate a very low entropy, and comparing back to the predictions for the simplified labels, these were the same areas where there were noticable amounts of a consistent mix of labels.</p>
<h2 id="timing-of-gp-vs-dm">timing of GP vs DM</h2>
<p>Given that the aim of approximation was to overcome the data limitations of a standard GP, we would want to see the different in run times for the a GP and its approximation, as well as the Dirichlet multinomial as well. Training of a generalised product of GP experts is considerably faster than a GP, as matrix inversions are limited to matrices of 200x200, compared to the GP that needs to invert a 4700x4700. Because of the overhead of a large number of experts though, as the matrix operations can be optimised, predictions for the approximation take longer than for the standard one, but is still much faster. The Dirichlet multinomial, on the other hand, does not involve the expensive steps that GPs do and only need top optimise over their parameter space - their speed is only in the seconds and are instantaneous compared to either the GP or its approximations.</p>
<h1 id="discussion">Discussion</h1>
<p>As seen earlier, the cross validation scores did not directly reflect in the quality of the predictive maps - one contributor to this may have been the fact that there was a significant class imbalance. However, without the opinion of an ecological expert, it is difficult to determine whether the cause may have been intrinsic in the data - and whether in the case of Scott Reef, bathymetry data and derived properties such as roughness and slope alone were enough to be able to clearly differentiate between habitats. The variance on the predictions suggest that this may be the case, as it was quite high for each of the most probable labels throughout the query space.</p>
<p>The resultant maps are also difficult to evaluate both quantitatively and qualitatively without input from marine biologists or similar on whether the patterns that emerge between habitats and their co-existence are plausible or not. Because of these missing sources of knowledge, the analysis on the results are restricted to interpreting the available data and predictions numerically, and only being able to make broad observations as to what they imply.</p>
<h1 id="conclusion">Conclusion</h1>
<p>In this thesis, we used approximation methods for Gaussian processes to show that they were a viable approach to scale probabilistic methods to larger datasets, without a loss of quality or usefulness in the resultant data compared to other methods. We then proposed fully utilising the label counts over all the habitats in the original data by using Dirichlet multinomial regression, so that predictions provided a distribution over labels, rather than a single label alone. This output then allowed easy extraction of underlying information of the habitat such as biodiversity that would require more expensive post-processing with other single-output classification methods.</p>
