\chapter{Literature Review} \label{chap:litreview}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OVERVIEW %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
In this chapter, an overview of will be provided of what benthic habitat mapping is and the general steps involved in data collection, followed by a review of some of most commonly used approaches when performing benthic habitat mapping. The practice of benthic habitat mapping precedes the rapid developments in machine learning methods in recent history, and as such, early attempts would naturally have involved manual predictions based on available information that would be subject to the biases of experts involved in the process. It is thus expected that given the same raw data, different experts who have had varying experiences in their field would come to different conclusions. 

This phenomena was observed in a geoscience related study \citep{bond07}, where over 400 individuals with geoscience backgrounds were asked to assess a synthesised seismic image, with just under 25\% correctly identifying the `true' tectonic setting and the three main fault strands. Interestingly however, (inexperienced) students were as likely to give incorrect responses as those with over 15 years' experience, where the latter often drew conclusions linked to the area that the held expertise in. Early efforts to create benthic maps followed this trend, where the lack of more formalised approaches meant experts would use the available data to extrapolate habitat maps based on the understanding that they had. This points to the variability of expert-driven modeling of natural environments such as benthic habitats, and the need for data-driven techniques, where expert input can be used as a supporting source of information rather than the only, or dominant one. 

\section{Benthic Habitat Mapping Overview}

The process of benthic habitat mapping involves three key steps that the large majority of all studies in the area go through\citep{ozcoasts}. In this section, we will give a brief overview of each of these steps, along with common procedures involved.
\begin{itemize}
    \item \textbf{Habitat Characterisation} - extracting properties of the environment such as rugosity (roughness), aspect (direction of slope), depth
    \item \textbf{Habitat Classification} - grouping the raw information about the environment into categories, such as sand, granite, etc.
    \item \textbf{Habitat Mapping} - using classifications with the larger scale bathymetry data to extrapolate habitat maps 
\end{itemize}

%%%%%%%%%%%%%%% Habitat Characterisation %%%%%%%%%%%%%%%
\subsection{Habitat Characterisation}
If high resolution, multimodal\footnote{Multimodal data refers to the different \textit{information} that resides within it - e.g. an area of benthic terrain can be represented by a single photograph, or numerical representations of its physical properties.} data for the entire ocean's benthos was easily obtainable, creating benthic habitats maps for any given area would be not be an incredibly difficult task. As this is prohibitively expensive, the altnerative is instead collecting relatively large amounts of economically obtainable information such as bathymatery data, and comparatively fewer samples of data that are costly to collect such as images (so that a relationship can be modeled between them, to be explained below). This subsection provides a brief summary of data collected and methods used to do so.

\paragraph{Remote-sensing data}
As exhaustively exploring Earth's ocean floor with underwater vehicles to capture it visually as well as all its physical properties is an infeasible task, compromise is required so that modest amounts of data can be collected economically, whilst still being informative. Remote-sensing data is thus used, usually obtained from acoustic backscatter methods, which involes the firing of sound waves towards the benthos, where their frequency and strength upon returning is used to deduce the physical properties of the area from where it rebounded, such as depth, slope, and roughness. More modern acoustic backscatter collection methods like Multibeam echosounders (MBES) are gradually superceding older ones such as Single-beam echosounders (SBES)\citep{calvert15} due to the \textit{cone} of the beam angle of the sound pulse in MBES being of a considerably smaller angle to reduce the amount of coarse, repeated data needed to try and resolve the true properties contained. \citep{cjbrown11}. At the same time, it maintains a larger swath angle, allowing data to be collected at a fast rate.

\begin{figure}[H]
    \includegraphics[scale=0.8]{sbes_mbes.png}
    \caption{Rough comparison of SBES with MBES \citep{braveheartmarine}}
    \label{fig:sbes_mbes}
\end{figure}

As pointed out in \cite{calvert15}, bathymetry data alone is not enough and should not be the sole basis on which habitat maps are formed, and as such, more conclusive information is required to make predictions in these studies.

\paragraph{Truthing Data}
One of the more common methods to be able to obtain a sufficiently large truthing data set (but small relative to areas with remote-sensing data) are videos or images, generally by autonomous means, where images have the advantage of not requiring the additional post-processing that video does to extract the images. Technology to send unmanned vehicles to the benthos and capture visual data is a relatively recent development, and prior to that, it was more common to send divers on manual expeditions that involved actually collecting sediment samples that were used to identify what the habitat at a particular location may have been. 

% The advantage that be provided , however, is the redundancy in data points ~\citep*{rattray14}  - but there is extra cost in time required to convert videos into the needed images (pre-proessing before feeding into algorithms for habitat mapping), an area that is in itself worth of research within the field.~\citep*{lucieer13}
% 
% \paragraph{Other data}
% \todo{(why is water column correction important when correlating images with seagrass standing crop?)} Other data that is less common, but also used to map habitats, is patterns in the water movement (such as tidal currents, wave action)~\citep*{cjbrown11} in the column of water above the area of benthos being mapped - a feature that provided useful input in arriving at an accurate benthic habitat map (in addition to sediment analysis).~\citep*{snelgrove94} Other sources such as UNESCO have also verified the importance and significance of using water column correction techniques to obtain more accurate habitat maps, particularly when correlating images with segrass standing crop. \footnote{Unesco.org. (2016). Water column correction techniques. [online] Available at: http://www.unesco.org/csi/pub/source/rs10.htm} 

%%%%%%%%%%%%%%% Habitat Classification %%%%%%%%%%%%%%%
\subsection{Habitat Classification}

\todo{more in-depth focus here, what kind of supervised/unsupervised ML algorithms are used for classification?} Almost all studies use \textit{in situ}\footnote{in situ, in a biological context, refers to the precise spot in which something occurs. In a habitat mapping context specifically when referring to truthing data, it simply means, in the case of bathymetry and image data, images taken at the exact spot corresponding to a particular bathymetry sampling location.} 'truthing' data to complement the acoustic data to be able to build a model between the acoustic data and truthing data (creation of these models are explained in following sections). However, we need to know the labels of this data considering that the final goal is to create a habitat map, where any one habitual zone is given its prospetive label - to do this, we also need to label the clusters of truthing data. These categories may be, for example, `bedrock covered by discontinuous seagrass cover', `Maerl interspersed with sand and gravel', `superficially coarse sand to fine gravel covered by dense patches of seagrass', etc.~\citep{micallef12}. Given physical sediment truthing data, the process of labeling locations is inevitably a manual one, but this is not the case when dealing with image data. The two overarching ways to perform this classification with images are using supervised and unsupervised algorithms.

Studies have used both unsupervised clustering and supervised clustering (classification) to label the truthing data for the model-fitting stage. Often, there may be large amounts of visual data, beyond that which any human or even team can reasonably, manually cluster - and as such, unsupervised algorithms are first used to create these clusters, after which an expert may be brought in to verify/simplify (or otherwise) the resulting clusters. One possible method in the unsupervised category is to use hierarchical clustering as seen in \citet{pizarro09}. This is an approach in which a layered tree\footnote{the tree described here is formally known as a \textit{dendrogram}} is formed where the two nearest clusters (based on a pre-deinfed distance metric) are grouped together to form a larger cluster, with a base case where every point is its own cluster. The distance metric used in this particular study was the Kullback-Leibler convergence between points, where each was attributed to a distribution of a set of features, including properties such as saliency that was calculated using colour and texture of the image, as well as colour histograms of comprehensively normalised images.

\begin{figure}[H]
    \includegraphics[scale=0.6]{pizarro-dendrogram.jpg}
    \caption{Dendrogram of images from  \citet{pizarro09} using Kullback-Leibler convergence distances between feature distributions}
    \label{fig:sbes_mbes}
\end{figure}

\subsection{Map Creation}
The final step is map creation, which many papers related to benthic habitat mapping focus on - and also where the most variation occurs in terms of the method used. The various approaches used can be categorised into two broad categories. The first is a top down approach whereby the classification of the habitat characterisation data is validated (or otherwise) with the truthing data, and the second is a bottom up approach where the characterisation data is similarly clustered into classes, but not to directly represent a particular habitat - instead, the aim is to find a relationship between the acoustic data clusters and the truthing data clusters which we can model. Using this model, we can then extrapolate the acoustic data which doesn't have corresponding truthing data to create the habitat map.~\citep{ahsan11} We will explore this aspect more when looking at how the mapping process has evolved over time and the improvements that it has brought about.

%%%%%%%%%%%%%%% Non-ML Habitat Mapping %%%%%%%%%%%%%%%
\section{Non-Machine Learning Approaches} 
\todo{this section should be part of the previous one (Map Creation)} While the majority of modern papers in benthic habitat mapping employ machine learning techniques for map creation, this doesn't exclude those that do not from providing useful information and insight. An important study was undertaken in 2001 that employs relatively basic statistical analysis, employing different forms of variance as its main tool of analysis. ~\citep{kostylev01}, at the time (and in fact, even now) integrated more sources of data together than most other studies undertaken - multibeam bathymetric data, geoscientific data, seafloor photographs, habitat complexity, and relative current strength. Rather than drawing broad conclusions about the effectiveness of a collection of tools in creating habiatat maps, deeper analysis is done on subsets of the data to attempt to clarify some of the complexities and intrinsic properties of benthic habitats and ecosystems themselves. Although little is done to address and verify accuracy of the actual results/map in this paper, it provides value through the analysis of variance and covariance performed on and between different benthic/marine properties, establishing relationships typically taken for granted or ignored. For example, it is established that while sediment type contirubted heavily to a higher taxonomic group count, there was little relationship between sediment type and depth. However, this only indicates that there is no linear relationship between the two, and doesn't necessarily preclude a non-linear relationship between them, perhaps with the inclusion of other parameters as well. In particular, Kostylev establishes that gravel subrates are more abundant with varying taxonomic groups than their sand counterparts.

Certain organisations, government bodies/etc. will also provide guidelines outlining the classification proess. For example, the European Nature Information System website and the Australian Government's 'Interim Marine and Coastal Regionalisation for Australia'\footnote{Unesco.org. (2016). Water column correction techniques. [online] Available at: http://www.unesco.org/csi/pub/source/rs10.htm} both provide classification schemes for people creating habitat maps or other similar efforts.

Such findings provide useful insights for future studies that will allow a better assessment of data, or to be able to apply initial assumptions in obtaining better results. However, constantly seeking a deeper understanding through a proportionally increasing amount of sampling creeps towards 'exploring' the entire Earth's oceans manually. To obtain economically feasible yet reliable predictions from the limited data that we have, we need to employ machine learning techniques to fully utilise the information that we gather.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Evolution of Map Creation Methods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Machine Learning in Benthic Habitat Mapping}
As benthic habitat mapping covers a diverse range of discplines, namely "marine biology, ecology, geology, hydrography, oceanography and geophysics"~\citep{cjbrown11}, in addition to statistics and machine learning, it is logical that it would take considerable effort and vast resources to give each relevant discpline an equal, and large amount of attention within any single study. Thus, different papers can rely on collective findings of others to launch their own research and look further into particular lines of inquiry, start new ones altogether, or evaluate effectiveness of methods used in the field/etc. Within benthic habitat mapping, one prevalent line of inquiry is how to create more accurate, higher quality maps by employing machine learning techniques. For the remainder of this review, we will be revisiting common machine learning techniques and their application in the various stages of benthic habitat mapping along with the benefits they provide.

\subsection{Deterministic Machine Learning Algorithms}
In this section, we will review some machine algorithms that can be used in benthic habitat mapping processes - whether that be in the initial clustering stages of (ideally) independently gathered datasets such as acoustic backscatter data and collections of high resolution images, or the actual classification of 'new' (or testing) data in determining their predicted habitat classes.

\paragraph{Multinomial Logistic Regression}
Multiple Logistical Regression is one of the more basic machine learning algorithms that can be used to predict habitat classes, and falls under the 'supervised learning' category as we have the 'output' for the feature vector in the intial data. Regression, broadly, involves the estimation of relationships between variables, and logistic regression involves the prediction of likelihood of class membership given a number of variables (that are assumed to have low collinearity). This only applies to domains with two classes, however - to use this technique for classification where we have an unbounded (though usually still relatively low) number of classes, we need to use multinomial logistic regression, which is able to account for more than two distinct, unordered (i.e., sand vs. mud has no relative ordering) classes, where class membership is predicted using maximum likelihood estimation (MLE), similarly to logistic regression. However, the difference is that whereas logistic regression only requiring a single logit function as its nominal variable is dichotomous, multinomial logistic regression requires comparison between $k-1$ (where $k$ is the number of possible dependent variables) logit functions. 

Even though ~\citet{caruana06} show that logistic regression methods achieve on average worse results than most other approaches available, it recognises that in certain cases the models that perform most poorly on average still display exceptional performance, and as such, this method is still worth exploration and experimentation. In particular, ~\citet{belanger12} used multinomial logistic regression across temperature, salinity, and productivity to correctly predict class membership by a margin of 23-84\% more than by pure chance. This is equivalent to an improvement of 1-2x compared to a random guess, which taken at face value would suggest that logistic regression is an undesirable choice of algorithm for this problem domain.

\paragraph{Random Forests}
In contrast to logistic regression, random forests were shown in ~\citet{caruana06} to be state of the art, only just falling short of boosted decision trees after callibration. Random forests are an ensemble method, meaning that it uses a collection of estimators, before aggregating their results to obtain some sort of average. The aim of this is to minimise the variance and hence error that any single one of these estimators would otherwise result in. 

From the initial dataset, some number $B$ is chosen which represents the \textit{number} of trees to build (as a part of our random forest), after which, $B$ random, unique subsamples of the full dataset are taken. Within each decision tree in our random forest, some constant number $m$ of features is taken at each node of the tree, such that the split at each node only takes into account the $m$ randomly chosen features. Each of the decision trees in our forest will hence have a 'result' (that may be a class or some continuous value). Typically, the final decision of the random forest will be made by a vote count for classification, and an average of each decision tree's result in regression problems.

As random forests are a method that is low in complexity but provides very good results on average, we can see that it is used in quite a few studies (~\citet{lucieer13}, ~\citet{seiler12}, ~\citet{hasan14}), where the random forest classifier provided the best results over other methods relating to at least a significant subset of the explored data. However, ~\citet{lucieer13} found that while random forest classifiers were most able to classify substratum and rugosity, K-nearest neighbour classifiers most accurately classified sponge structure classes, pointing to the need to do a more systematic comparison of different methods in benthic habitat mapping. A further advantage to using random forests as pointed out in ~\citep{hasan14} is that it can provide insight into which features were more important than others, which can aid future studies to be more successful and efficient by focusing more efforts towards collecting the most influential data. The success met with using random forests make it a good benchmark to compare against for future work that aim to develop methods to create more accurate benthic habitat maps than has been done before.

\paragraph{Multi-class Support Vector Machines}
Although support vector machines fell outside the top three overall supervised learning algorithms in terms of performance (accuracy), their non-parametricity could potentially be of benefit given that our knowledge of the complex relationships between elements of benthic habitats are limited. Moreover, despite SVMs being rarely used anywhere in the field, they are "acknowledged to be very competitive descriminative classifiers in machine learning literature"~\citep{ahsan11}.

However, SVMs in their base form only support classification into two classes, requiring modification to the original algorithm to support more - an active area of research that has not found any single 'best' way to perform this algorithm extension yet. To do so, there are two, basic main approaches available, one being a \textbf{one vs. all} approach, and the other being a combination of all \textbf{one vs. one} approach. We can hence quite clearly see that given $C$ possible classes, the first would require $C$ separate classifiers, whereas the latter would require $\frac{C(C-1)}{2}$ classifiers~\citep{murphy12}. Multi-class SVMs were used in ~\citet{ahsan11} for illustrative purposes, using the one vs. one approach as per their use of LibSVM~\citep{chang11}, outperforming classification trees on certain datasets, and in limited cases (but not overall), Gaussian Mixture Models as well. Again, the mixed results would suggest that under certain conditions (such as size of the dataset and various properties of the data itself, some of which are not known before testing), use of a multi-class SVM could provide a useful benchmark to some extent.

%%%%%%%%%%%%%%% Probabilistic Methods %%%%%%%%%%%%%%%
\todo{EVERYTHING FROM HERE LIT REVIEW SHOULD BE REMOVED}
\subsection{Probabilistic Methods}
The classifications being made regarding benthic habitats naturally involve uncertainty, as we are still learning the relationship between differnt characteristics of benthos with the varying communities of fauna and flora that reside there. Whilst guessing the most likely class for a particular domain deterministically has its practical applications, it is arguably more \textit{natural} to represent the uncertainty ~\citep{rasmussen06}. As our understanding of marine environments is still quite weak ~\citep{un04}, it is debatable whether deterministic results are always appropriate when being used to make high level management decisions relating to marine environments. While deterministic methods will create a model that attempts to explicitly account for all variables, probabilistic models deal with joint distributions over all the variables. As we need to better understand "the complexities of coastal system functioning rather than simplfying and scaling down the system into smaller components" ~\citep{diaz04}, this feature can be esepcially valuable seeing as there is simply not enough 'expert knowledge' to adequately, explicitly model the relationship across a range of variables.

\paragraph{Illustrative Example} \todo{not an illustrative example - give some actual figures/graph of crossover point} A simple example of this can be seen when comparing the deterministic approach of a logistic regression classifier, with the probabilistic Naive Bayes classifier. Starting from no data, up until a certain threshold, a Naive Bayes (NB) classifier will actually provide a more accurate classification as it approaches its comparatively higher asymptoptic quicker, after which point, one there is sufficient data, the logistic regressor will provide the better results ~\citep{ng04}. In this simplified example, an analogy can be drawn where the data used up to the threshold when the NB classifier performs better represents a lack of knowledge about the data causing the logistic regressor to underperform, where as the continued addition of data represents more understanding (more data points) of the domain, allowing logistic regression to then outperform the NB classifier.

\paragraph{Gaussian Mixture Models} Gaussian mixture models (GMMs) are parametric models that "model the distribution of data as a set of clusters, where each cluster is a multivariate Gaussian"~\cite{ahsan11}. In this particlar paper, GMMs are compared with classification trees, which it is found to perform better than in most cases, but were also predicted classes from unseen data with higher certainty than discriminative methods. This is because of its generative nature that accounts for the distribution of bathymetric features, allowing it to model the joint distribution of the classes as well as features. Moreover, each \todo{(function = distribution)}Gaussian function within the model has its own mean and covariance matrix, which also contributes to its powerful modeling ability. However, the use of GMM may have been hindered by the dimensionality of the data - while only five properties were measured, each was calculated for a varying number of scales for the input vector, meaning the 'features' were at least some multiple of five. As this exceeds the recommended six dimensions for use with GMM, application to a very large dataset may be beyond reasonable computational ability.\footnote{Nickgillian.com. (2016). GMM Classifier â€” NickGillianWiki. [online] Available at: http://www.nickgillian.com/wiki/pmwiki.php/GRT/GMMClassifier}. To avoid this, the feature vector may have to be truncated to contain the bathymetric properties for only one particular scale at a time.

\paragraph{Using Gaussian Processes} A recent study used probabilistic methods to develop a mapping between the clustered acoustic data to continuous cluster probabilities, as opposed to discrete cluster labels, thus representing the certainty of the results obtained. Using Gaussian Processes which do not inherently support classification, ~\citet{bender12} extended the probabilistic least squares classifier to retain the information regarding certainty of class membership that exists during the classfication process, rather than discarding it in the traditional method. By evaluating the probabilistic results of PTLSC by comparing its results with the actual cluster probabilities obtained in the classification of the images via an unsupervised variational Dirichlet process model, it was shown that the PTLSC method performed better than a PLSC trained directly on the discrete cluster labels in terms of accuracy, mean squared error, and mean variance as well. This demonstrates that while both PTLSC and PLSC err in their predictions when dealing with the transition different boundaries, by maintaining probabilistic information in the PTLSC, it is able to make slightly better judgements in such cases.

\paragraph{Gaussian Processes and large datasets} However, Gaussian processes involve a matrix inversion process that requires an O($n^3$) operation which does not scale well with large datasets. To overcome this whilst reaping the benefits of Gaussian processes, ~\citet{bender12} extracted subsets of the original dataset on which to perform analysis - a small, randomly chosen portion from three Gaussians, of the initial millions of observations. While this has still provided a high accuracy for all methods tested, there is likely information to be gained by being able to use a considerably larger portion of the dataset. To do this, a method would be required to generate sparse covariance matrices through approximations ~\citep{bickel08}, or use of functions that guarantee sparseness as a property~\citep*{melkumyan09} - something that can be explored in future work. To illustrate how the obstacle can be overcome, the latter paper describes a method whereby, rather than inverting the covariance matrix in its raw form, a threshold is calculated at which point, rather than observing the normal 'tapering' off of covariance values, they are simply set to zero beyond that point. This will result in a significant portion of the covariance matrix being populated with 0s, at which point inversion of the sparse matrix can be performed for which there are known efficient methods. However, there have been more ways of sparse approximation GPs that other studies have explored.

\paragraph{Sparse Approximation Gaussian Processes}

\todo{m isn't clarified here, plus n<m is incorrect, should be m<<n} 

\todo{lots of things here have become irrelevant (talks about 'optimal' methods that aren't implemented/included in experiments - limit to relevant ones, i.e. the GP ensemble methods}

It is of importance that a number of methods of dealing with sparse approximation of GPs are taken into account if the aim is to deal with large GPs in the inversion step. ~\citep{candela05} explores exactly this, immediately discounting the "subset of data" (SoD) method as being non-competitive due to it not being able to represent the original data to a reasonably accurate enough extent, though we have seen that this was the approach taken in ~\citep{bender12}. As all the different methods (bar SoD) have a complexity of $O(nm^{2})$ where $n$ is the size of the data, and $n<m$, the authors notably point out that no gross approximations should be made as more competent methods are computationally equivalent, and as such point towards their notes on future work to outperform the existing state of the art. As such, we would wish to explore combining the \textbf{Partially Independent Training Conditional approximation} with "the most powerful selection method for the inducing inputs." 

