\chapter{Probabilistic Habitat Mapping} \label{chap:gps}

The methods of habitat mapping explored until now were mostly deterministic ones, where predictions were absolute, and as such did not provide a \textit{level of confidence} in the predictions made, or in other words, probabilistic output. The partial exception to this was logistic regression, though the probabilities provided by it are absolute, with no variance to indicate confidence. Regardless, as a parametric method, the complexity of a logistic regressor must be defined beforehand, whereas a Gaussian process in simple terms allows the data to 'speak for itself`. More formally, this refers to a Gaussian process' non-parametric nature, meaning the data is incorporated directly into the model where new data can increase the confidence of the model.

In this chapter, we will look at Gaussian processes as a technique to generate predictive habitat maps. We begin by going over the basics of Gaussian process regression, and how a small extension/post-processing step extends it to allow Gaussian process classification. To train the hyperparameters of a Gaussian process, leave-one-out cross validation (LOO-CV) was used. They also need to define a \textit{kernel} that governs the similarity between any two points, forming the full covariance matrix that described the relationship between all pairs of points. The kernel chosen was the squared exponential kernel, one of the most commonly used for Gaussian processes. Note that detailed proofs and derivations are not covered here, and interested readers should consult Rasmussen and William's Gaussian Processes for Machine Learning book ~\citep{rasmussen06} for a definitive guide to all things Gaussian process related. In particular, Chapters 2 and 5 are of the most relevance, as they detail Gaussian process regression, and model selection and adaptation of hyperparameters respectively.

% $\mathbf{f_*}$ - predictive function outputs corresponding to input data $X_*$
 \section{Gaussian Process Regression}\label{chapsec:gpr}

Compared to deterministic methods like linear regression\footnote{Strictly speaking, linear regression is a Gaussian process with a linear kernel - but due to its simplicity, and that Gaussian processes are not usually explained by weight coefficients but by their hyperparmeters, we are juxtaposing linear regression to Gaussian processes here as a simpler approach.} that explains data by optimising $\mathbf{y=X\beta} + \epsilon$, where $\mathbf{y}$ are the response variables, $\mathbf{X}$ are the input variables, and $\mathbf{\beta}$ are the regression coefficients, Gaussian process regression takes a Bayesian approach by adjusting probabilities when given more information (input data), and performs inference over functions.

We define a Gaussian process on input $\mathbf{x}$ to have a mean function $m$ and covariance function $k$ (or in other words, the kernel), where $\mathbf{x}$ and $\mathbf{x'}$ are the training and test inputs respectively:
\begin{equation}
f(x) \sim GP(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x'}))
\end{equation}

The chosen kernel is the squared exponential, represented by the covariance function between points $p, q$, where $\mathbf{x_p, x_q}$ are the vector of features at each point is thus given by:
\begin{equation}\label{eq:simplegpcov}
    cov(f(\mathbf{x_p}), f(\mathbf{x_q)}) = k(\mathbf{x_p, x_q}) = \exp(-\frac{1}{2}|\mathbf{x_p}-\mathbf{x_q}|^2)
\end{equation}

The free parameters involved in the squared exponential kernel are the length scales $l$, signal variance $\sigma_f$ and noise variance $\sigma_n$, and need to be optimised to perform predictions on unseen data.
\begin{equation}\label{eq:fullgpcov}
    k\mathbf{(x_p, x_q)} = \sigma^2_f \exp(-\frac{1}{2l^2} (\mathbf{x_p-x_q})^2) + \sigma^2_nI
\end{equation}
where $\sigma_f$ is the variance in the training data, and $\sigma_n$ is the variance of the Gaussian noise. The length scales $l$ are not a single variable as the equation may suggest, but a vector of length scales equal in length to the number of dimensions in the inputs $\mathbf{x}$. If $l$ was simply a vector of 1s, which would give every feature in the input space equal weighting, but it is not likely in a real world dataset for every feature to have equal importance. This is what the length scales account for - by tuning $l_i$ for each feature $i$, the model can learn during the fitting process which features are important, and which ones are not. 

 The equations for the predictive means and variances then incorporate the covariance function, and hence the hyperparameters, as follows:
\begin{equation}
    \bar{f_*} = \mathbf{k_*}^T(K+\sigma_n^2)^{-1} \mathbf{y}
\end{equation}
\begin{equation}
    \mathbb{V}[f_*] = k(\mathbf{x_*},\mathbf{x_*}) - \mathbf{k_*}^T (K+\sigma^2_n)^{-1}\mathbf{k_*}
\end{equation}
where $K = K(X, X)$ is the covariance matrix over all training points, and $\mathbf{k_*} = \mathbf{k(x_*}, X)$ is the covariance between a single test point with all training points.

% To allow simplifications of notation in the following equations, we define some abbreviations related to \autoref{eq:fullgpcov} depending on what data is involved in the covariance matrix. 
%  
%  \todo{(lay out these abbreviations nicely)}
%  To indicate the full covariance matrix over training points: $$K = K(X, X)$$
%  To indicate the full covariance between training points and test points: $$K_* = K(X, X_*)$$
%  To indicate the covariance between a single test point with all training points: $$\mathbf{k_*} = \mathbf{k(x_*}, X)$$


A few illustrative plots have been included below, with a Gaussian process performing inference on several data points but with different hyperparamters each time, to highlight the effect that they have on its predictive ability.

\section{Leave-One-Out Cross Validation}

\todo{(expand section)}

To train our data, we chose the extreme case of of cross-validation for model training, where the number of folds used, $k$, is equal to the number of datapoints. By optimising over the sum of cross-valiated log likelihoods, it is no longer strictly only assessing the log marginal likelihood, instead acting as more of a pseudo-likelihood. Directly optimising over the marginal likelihood provides the probability of observed data \textit{given model assumptions}, whereas the cross-validation approach provides the log predictive probability estimates independent of the fufilment of said model assumptions. The latter case is preferrable here as biological experts were not consulted for the duration of the study, meaning some assumptions could have been tuned more accurately if external help was available.

The log probability ommitting training case $i$

$$\log p(y_i|X, \mathbf{y_i}, \theta) = -\frac{1}{2}\log\sigma^2_i - \frac{(y_i - \mu_i)^2}{2 \sigma^2_i} - \frac{1}{2}\log2\pi$$

$$ L_{LOO}(X, y, \theta) = \sigma^n_{i=1} \log p(y_i, X, \mathbf{y_i}, \mathbf{\theta})$$

LOO-CV predictive mean and variance
$$\mu_i= y_i - [K^{-1}\mathbf{y}]_i / [K^{-1}]_{ii} \text{ and } \sigma_i^2 = 1/[K^{-1}]_{ii}$$

partial derivatives with respect to the hyperparameters
$$\frac{\partial{u_i}}{\partial{\theta_j}} = \frac{[Z_j \alpha]}{[K^{-1}]_{ii}} - \frac{\alpha_i[Z_j K^{-1}_{ii}]_{ii}}{[K^{-1}]^2_{ii}}$$
$$\frac{\partial{\sigma_i^2}}{\partial{\theta_j}} = \frac{[Z_jK^{-1}]_{ii}}{[K^{-1}]^2_{ii}}$$

where $\alpha = K^{-1}\mathbf{y}$ and $Z_j = K^{-1} \frac{\partial{K}}{\partial{\theta_j}}$


\section{Gaussian Process Classification} \label{chapsec:gpc}

To perform Gaussian process classification, multiple Gaussian process Regressors are used for each possible label using a one-vs-all approach as in Example \autoref{ex:one-vs-all}, and every label at each data point is assigned both a probability and variance. To ensure the predictions at each point for each label is propertly constrained to be in the range $[0,1]$, we again pass predictions through the logistic sigmoid function (\autoref{eq:logistic}).

\todo{(expand)}

% \section{Subsampling for Gaussian Process experiments}
% \todo{(may need to take this section out - GPy wrapper deals with thousands of points no problem, no longer need to subsample with only 4000 after downsampling the data)}
% 
% Due to the $O(n^3)$ complexity of training a Gaussian Process Classifier, using all $16502$ points was infeasible, so it was necessary to use only a subsample of the training data. As can be seen in the above histograms \todo{(reference the figure instead. may need to combine them into one)}, the distribution of classes in both the simplified and non-simplified versions was very uneven. As a result of this skew, randomly sampling the the training data to fit our GP classifier against resulted in worse results than samplying an equal \textit{number} of points for each class. To obtain a reasonably well-performing set of 1000 points (the number chosen to obtain a balance between performance and time required), 10-fold cross validation was performed on random subsets of this size. To obtain the 1000 datapoints, both stratified sampling, as well as obtaining ratios of labels matching those in the training set were used. After 200 runs, the set with the best performance was used for the remaining GP experiments.

\section{Gaussian Process Approximation} \label{chapsec:gp-approx}

However, significant limitations exist in terms of the number of training datapoints used when using a single Gaussian process classifier. To counter this, there are approximation methods which allow more points to be used whilst not expending more time. There are two common ways to do this, the first being using inducing inputs, where approximations are made such that neither extra time or computational power is needed to encapsulate the information of more points. The second are \textit{ensemble} methods - combining the results of several independent Gaussian processes trained in parallel (at the expense of more computational power), and was the one tested as a part of this study.

\subsection{Product of Experts and Variations}
\todo{(EXPAND explanations in this section)}

Product of GP Experts
\begin{equation}
    \mu_*^{poe} = (\sigma_*^{poe})^2 \sum_k \sigma_k^{-2} (\mathbf{x_*}) \mu_k (\mathbf{x_*})
\end{equation}
\begin{equation}
    (\sigma_*^{poe})^{-2} = \sum_k \sigma_k^{-2} (\mathbf{x_*})
\end{equation}

Generalised Product of GP Experts
\begin{equation}
    \mu_*^{gpoe} = (\sigma_*^{gpoe})^2 \sum_k \beta_k \sigma_k^{-2} (\mathbf{x_*}) \mu_k (\mathbf{x_*})
\end{equation}
\begin{equation}
    (\sigma_*^{gpoe})^{-2} = \sum_k \beta_k \sigma_k^{-2} (\mathbf{x_*})
\end{equation}
The value of each $\beta_k$ is flexible, but as scaling Gaussian processes to large datasets isn't the primary focus of this study, we simply set each $\beta_k$ to $\frac{1}{M}$, where $M$ is the number of experts, as suggested in \citep{deisenroth15} to maintain reasonable margins of error.

\subsection{Bayesian Committee Machines and Variations}

Bayesian Committee Machine
\begin{equation}
    \mu_*^{bcm} = (\sigma_*^{bcm})^2 \sum_{k=1}^M \sigma_k^{-2} (\mathbf{x_*}) \mu_k (\mathbf{x_*})
\end{equation}
\begin{equation}
    (\sigma_*^{bcm})^{-2} = \sum_{k=1}^M \sigma_k^{-2} (\mathbf{x_*}) + (1-M)\sigma_{**}^{-2}
\end{equation}
where $\sigma_{**}^{-2}$ is the prior precision of $p(f_*)$, which itself is the inverse of the prior variances.

Robust Bayesian Committee Machine
\begin{equation}
    \mu_*^{rbcm} = (\sigma_*^{rbcm})^2 \sum_k \beta_k \sigma_k^{-2} (\mathbf{x_*}) \mu_k (\mathbf{x_*})
\end{equation}
\begin{equation}
    (\sigma_*^{rbcm})^{-2} = \sum_k \beta_k \sigma_k^{-2} (\mathbf{x_*}) + (1-\sum_{k=1}^M \beta_k) \sigma_{**}^{-2}
\end{equation}
where each $\beta_k$ follows the same rules as for the Product of Experts and its variations.

\citep{deisenroth15}

\todo{(gaussian mixture model paper - compares GMMs with other models such as CTs, SVMs, and entropy of predictions)}

\section{Summary}

In this section, we explored the probabilistic capabilities of Gaussian processes regression, and how this translates into Gaussian process classification. As these presented very restrictive limits on the size of data that can be worked with, methods to estimate them by breaking down the algorithm into emabarrasingly parallelisable chunks were explored, along with how they compare in terms of performance. It would amiss at this point not to address the existence of multi-output Gaussian processes that can work with correlated data (as the data in benthic habitat mapping would be - the habitats that co-eixst do not do so in isolation and no relation to each other), as explored in works such as \todo{(cite a few works)}. However, the multi-output that they deal with is over arbitrary ranges. Because in this particular domain, normalised predictions per point must sum to $1$ (or the original total label count at a point, in the case of training data), multi-output Gaussian processes do not enforce this constraint, and as such, their use was not explored in this study. Without a way to correctly model the multi-output data using Gaussian processes, other methods that are able to need to be explored.
