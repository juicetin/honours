\chapter{Results} \label{chap:results}

The results from the Experiments detailed in Chapter 4 are listed below. The range of possible class values in some cases have been stretched beyond the existing class labels so that values align between different outputs to allow for easy, direct visual comparison. Note that the results to the above experiments will include those of both non-downsampled and downsampled results, as well as the full set of 24 labels as well as simplified ones.

Due to the low occurrence of some labels in the original dataset though, they have ended up being ommitted in predictions - these are excluded from the colour schemes of the benthic maps generated, so that those that do occur can be given more distinct colours from one another as to better differentiate between the habitats of a map, as well as allow a consistent comparison of across different maps.

In this section, the performance of common machine learning algorithms, namely kNN, Logistic Regression, Random Forest, and SVM are explored first, to provide a comparison to the later, more complex algorithms.

% The associated generated maps from the experiments are also provided here, but proper evaluation of them, such as what habitat clusters and relationships can be gleaned, will be explored in Chapter 5. 

\pagebreak
\section{Deterministic Methods}

\todo{results here currently outperform the argmax of GPs and DMs...hmm}

\begin{tabular}{|c|c|c|c|c|c|}
    \hline
    Algorithm & 10F-CV F1 & 10F-CV Accuracy & Parameters & Data \\\hline
        KNN         &  0.7180212553324016  &  0.8752880347998457 &   n = 5  & simple labels \\
Logistic Regression &  0.21221134330881872 &  0.7374253253308372 &            & simple labels \\
   Random Forest    &  0.8103659316744121  &  0.9117076519281244 &            & simple labels \\
        SVC         &  0.21221261743619468 &  0.7374255088743278 &     OvA    & simple labels \\
        KNN         &  0.4711662374785644  &  0.6510716003156948 &   n = 5  &  full labels  \\
Logistic Regression &  0.06457891531653175 & 0.25912130389295746 &            &  full labels  \\
   Random Forest    &  0.6075895500970125  &  0.7263355175008718 &            &  full labels  \\
        SVC         & 0.012413092166172946 & 0.16549499843988033 &     OvA    &  full labels  \\
    \hline
\end{tabular}

\section{Gaussian Process Classification}

\todo{transfer all the results from markdown}

\todo{show more stratified results (not just even split) to show that even splits did better}

\begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    500     & Even       & GP     & 10F-CV & 10        & 0.86534    &                     &         \\
    500     & Stratified & GP     & 10F-CV & 10        & 0.80136    &                     &         \\
    1000    & Even       & GP     & All    & 1         & 0.87626    & Deterministic       & 0.56208 \\
    1000    & Even       & PoEGP  & All    & 5         & 0.80973    &                     & 0.47481 \\
    1000    & Even       & PoEGP  & All    & 200       & 0.80186    &                     & 0.47595 \\
    1000    & Even       & GPoEGP & All    & 5         & 0.80864    &                     & 0.51018 \\
    1000    & Even       & GPoEGP & All    & 200       & 0.80105    &                     & 0.47748 \\
    1000    & Even       & BCM    & All    & 5         & 0.80682    &                     & 0.48167 \\
    1000    & Even       & BCM    & All    & 200       & 0.80421    &                     & 0.48227 \\
    1000    & Even       & GPy    & All    & 1         & 0.87638    & RBF, EP (default)   & 0.57013 \\
    \hline
\end{tabular}

\todo{highlight areas with low/high certainty, etc. NOTE - investigate the areas with visually even splits of two labels - e.g. right-side arms of label 1,2, and smaller patches in the bottom left corner of label 0,3 - show that uncertainty about whether those areas are label 1 or 2, 0 or 3 respectively, is (should) be high, and that taking argmax for the sake of visual representation within a single image hides this information}

\section{Dirichlet Multinomial Regression}

% \begin{tabular}{|c|c|c|c|c|c|}
%     \hline
%     Algorithm & 10F-CV F1 & 10F-CV Accuracy & Parameters & Data \\\hline
%          DM         &  0.13802716811804644 & 0.37856057852908254 &            &  full labels  \\
%          DM         &   0.287405310254214  &  0.757925654489819  &            & simple labels \\ %     \hline
% \end{tabular}

\begin{tabular}{|c|c|}
    \hline
        Features & Root Mean Squared Error \\\hline
        Original features, using coordinates & 0.3360700584111156 \\
        Original features, not using coordinates & 0.33676707345298545 \\
        Quadratic projection, using coordinates & 0.19695170465540882 \\
        Quadratic projection, not using coordinates & 0.19726479764111976 \\
    \hline
\end{tabular}

In line with what was observed in the DM illustrative example in \autoref{table:toy_gm_vs_gp}, performance was better when taking the quadratic projection of the original features - though in this case, the improvement wasmuch more significant, namely by 41.4\% compared to the 6.5\% in the illustrative example.

As a means of effectively visuailsing the separate labels, we need to look at the normalised distribution of habitat classes for each label separately. This allows initial observations to be made of where certain labels are more abundant than others. Immediately, we can see the concrete advantages of using a multi-output model, as individual points no longer contain only a single label - it is now possible to see, for example, that the outreaching \textit{arm-like} portions on the right hand side of Scott Reef Central contain large areas of even splits of label 1 and 2, with trace amounts of label 0, but almost no label 3, which is otherwise the most frequently occuring habitat in the area \todo{(get the \textbf{actual} label names!)}.

\begin{figure}[H]
    \begin{minipage}{.45\linewidth}
        \centerline{\includegraphics[scale=0.62]{dm_simplelabel_heatmap_0.pdf}}
        \caption{label 0}
        \label{fig:dm_sl_hm_0}
    \end{minipage}
    \hfill
    \begin{minipage}{.45\linewidth}
        \includegraphics[scale=0.62]{dm_simplelabel_heatmap_1.pdf}
        \caption{label 1}
        \label{fig:dm_sl_hm_1}
    \end{minipage}
    \begin{minipage}{.45\linewidth}
        \centerline{\includegraphics[scale=0.62]{dm_simplelabel_heatmap_2.pdf}}
        \caption{label 2}
        \label{fig:dm_sl_hm_2}
    \end{minipage}
    \hfill
    \begin{minipage}{.45\linewidth}
        \includegraphics[scale=0.62]{dm_simplelabel_heatmap_3.pdf}
        \caption{label 3}
        \label{fig:dm_sl_hm_3}
    \end{minipage}

    \todo{fix the bar legends here}
\end{figure}

In the simple case where the labels are summarised down the basic four, we can see \todo{I can't remember what I was saying here...}

In contrast to the GP above where the uncertainty was greatest when there were even distributions of labels, it is expected that the DM would be confident that an even mix of labels exist in these areas. \todo{summarise and plot the variances}

\subsection{Biodiversity}
\todo{highlight areas with biodiversity, particular co-occurring species, etc.}
