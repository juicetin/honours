\section{Probabilistic Approaches (Single Output)}

In this section, we will add an extra layer of information to our models' outputs - the confidence of the label predictions made. When predictive variance at each point is given, a large variance would indicate a low level of confidence as the predicted value is any within a large range, whereas a small variance indicates a high level of confidence in a prediction, as the possible range of values is only a small one. For this, we need probabilistic models that naturally provide this desired variance in its predictions. In particular, as we saw in \Cref{chapsec:gpc}, Gaussian process classification is a good option for this.

\subsection{Gaussian Process Classification}

While f-scores and accuracy are still assessed via $10$-fold cross validation whilst using Gaussian process classification, we introduce another metric, area under the receiver operating curve, to make use of the fact that the one-vs-all Gaussian process classifier provides a \textit{likelihood} of each label's membership at each datapoint. This encapsulates that at any given point, predictions will (almost) never be 100\% certain - every single possible label, however unlikely, will have a probability assigned to it. 

Area Under The Receiver Operating Curve
\todo{TODO}

\todo{This round's acc: 0.2822085889570552, f1: [ 0.          0.46153846  0.29787234  0.47368421  0.          0.15384615
  0.34482759  0.05263158  0.22727273  0.4         0.28571429  0.28571429
    0.33333333], f1 avg: 0.25511038180964307, auroc: 0.41135738361342433, auroc avg: 0.41135738361342433}
\todo{This round's acc: 0.3936842105263158, f1: [ 0.          0.36734694  0.47787611  0.50943396  0.          0.30379747
  0.47727273  0.          0.27027027  0.44444444  0.14285714  0.375
    0.33333333  0.52173913  0.          0.85714286], f1 avg: 0.31753214883402126, auroc: 0.5498975366846471, auroc avg: 0.5498975366846471}
\begin{table}[H]
    \centering
\begin{tabular}{|c|c|c|c|c|}
    \hline
    AUROC & Accuracy & F-score & Labels used\\\hline
    0.85 & 0.83 & 0.53 & 4 labels \\
    0.55 & 0.39 & 0.32 & 24 labels \\
    \hline
\end{tabular}
    \caption{Gaussian proces 10-fold cross validation errors on full and simplified labels}
    \label{table:gperrs}
\end{table}

% \todo{May ditch this table}
% 
% \begin{tabular}{|c|c|c|c|c|c|c|c|}
%     \hline
%     No. points & Type of split & Type of GP & Number of runs & AUROC & Notes & F1-score \\\hline
%     500     & Even       & GP     &  10        & 0.86534    &                     &         \\
%     500     & Stratified & GP     &  10        & 0.80136    &                     &         \\
%     1000    & Even       & GP     &  1         & 0.87626    & Deterministic       & 0.56208 \\
%     1000    & Even       & PoEGP  &  5         & 0.80973    &                     & 0.47481 \\
%     1000    & Even       & PoEGP  &  200       & 0.80186    &                     & 0.47595 \\
%     1000    & Even       & GPoEGP &  5         & 0.80864    &                     & 0.51018 \\
%     1000    & Even       & GPoEGP &  200       & 0.80105    &                     & 0.47748 \\
%     1000    & Even       & BCM    &  5         & 0.80682    &                     & 0.48167 \\
%     1000    & Even       & BCM    &  200       & 0.80421    &                     & 0.48227 \\
%     1000    & Even       & GPy    &  1         & 0.87638    & RBF, EP (default)   & 0.57013 \\
%     \hline
% \end{tabular}
% \todo{(look at AUROC/AUC and log probabilities as well)}

\begin{figure}
    \includegraphics[scale=0.9]{gp4_allpreds.pdf}
    \caption{Gaussian process classification predictions over full query space for simplified labels}
    \label{gp4_allpreds}
\end{figure}

\todo{highlight areas with low/high certainty, etc. NOTE - investigate the areas with visually even splits of two labels - e.g. right-side arms of label 1,2, and smaller patches in the bottom left corner of label 0,3 - show that uncertainty about whether those areas are label 1 or 2, 0 or 3 respectively, is (should) be high, and that taking argmax for the sake of visual representation within a single image hides this information}

\todo{(talk about variance and extra probabilistic info to be gained)}

\todo{(maps of 4-label full predictions)}

\todo{(maps of all-label full predictions)}

\subsection{Ensemble Gaussian Process Approximations}

Although Gaussian processes provide the benefit of the possible variance for every prediction, the matrix inversion steps required and their $O(n^3)$ complexity prevents scaling of fitting the model beyond several thousand points (that even on high-end consumer hardware is impractically time-consuming), and predictions an order of magnitude above that. Using the naive Gaussian process, model fitting on the aggregated $4$-label data with $5000$ data points took over a day\todo{(double check this again!)}, with the predictions of the $500,000$ training points taking another day. It would be helpful to note that the formulation of Gaussian process classification used in this study (binary one-vs-all classifiers per label) means the complexity not only scales with number of points, but also in the number of possible labels, with each label requiring another underlying Gaussian process to account for it - for example, data containing $24$ labels would require 24 separate binary Gaussian processes. Due to hardware constraints, the $24$-label cases were not run to completion using the naive approach. As a point of reference, model fitting and predictions were timed for the basic Gaussian process, but only for a subset of the data, as shown below.

\begin{table}[H]
    \parbox{.85\linewidth}{
    \centering
    \begin{tabular}{|c|c|C{3cm}|}
        \hline
        Points & Labels & Time Taken (hh:mm:ss) \\\hline
        1000 & 4  &  00:00:37 \\
        2000 & 4  &  00:02:12 \\
        3000 & 4  &  00:11:53 \\
        4000 & 4  &  00:33:26 \\
        4700 & 4  &  01:15:41 \\
        1000 & 24  & 00:00:54 \\
        2000 & 24  & 00:07:03 \\
        3000 & 24  & 00:23:42 \\
        4000 & 24  & 00:39:43 \\
        4700 & 24  & 01:07:15 \\
        \hline
    \end{tabular}
    \label{table:gpensemble-results}
    \caption{Gaussian process model fitting runtimes, using gradually increasing number of points for both simplified and full-label cases}
}
\end{table}
\begin{table}[H]
    \parbox{.85\linewidth}{
    \centering
    \begin{tabular}{|C{2cm}|c|C{3cm}|}
        \hline
        Training points used & Labels & Time Taken (hh:mm:ss) \\\hline
        1000 &  4 &  00:01:23 \\ % (00:49:57??) 
        2000 &  4 &  00:04:18 \\
        3000 &  4 &  00:10:02 \\
        4000 &  4 &  00:17:35 \\
        4700 &  4 &  00:32:41 \\
        1000 & 24 &  \\
        2000 & 24 &  \\
        3000 & 24 &  \\
        4000 & 24 &  \\
        4700 & 24 &  \\
        \hline
    \end{tabular}
    \label{table:gpensemble-results}
    \caption{Gaussian process predictoin runtimes, using gradually increasing number of points for both simplified and full-label cases}
}
\end{table}

On the other hand, the benefits of Gaussian processes need not be sacrificed on account of this disadvantage of Gaussian processes, as approximations exist to break the original dataset into smaller chunks, allowing parallelisation and model fitting per smaller set of data that is only limited by the available hardware. As seen in \Cref{chapsec:gp-approx}, one such method is to use ensembles of Gaussian processes that allow trivial parallelisation. These experiments were carried out on \textbf{d2.8xlarge} Elastic Container (EC2) instances from Amazon, with the following specifications:
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Instance Type & vCPUs* & Memory(GB) & Physical Processor & Clock Speed \\\hline
        d2.8xlarge & 36 & 244 & Intel Xeon E5-2676v3 & 2.4Ghz\\\hline
    \end{tabular}
    \label{table:ec2specs}
    \caption{Amazon EC2 Instance Machine Specifications}
\end{table}

To illustrate the usefulness of these approximation methods, experiments were run to measure their accuracy and f-scores, but also the time needed to run them particularly compared to naive Gaussian processes and usefulness of the maps - providing a point of reference when looking at Dirichlet multinomials in the next chapter. The abbreviations of the ensemble methods will be used in the following tables: product of experts (PoE), generalised product of experts (GPoE), Bayesian committee machines (BCM), robust Bayesian committee machines (rBCM).

\begin{table}[H]
    \parbox{.45\linewidth}{
        \centering
    \begin{tabular}{|C{2cm}|c|c|c|c|}
        \hline
        Ensemble method & F-score & Accuracy & AUROC \\\hline
        PoEGP & 0.35 & 0.76 & 0.71 \\
        GPoGPE & 0.33 & 0.75 & 0.69\\
        \hline
    \end{tabular}
    \caption{Gaussian process approximation results for simplified (4) labels}
}
    \hfill
    \parbox{.45\linewidth}{
        \centering
    \begin{tabular}{|C{2cm}|c|c|c|c|}
        \hline
        Ensemble method & F-score & Accuracy & AUROC \\\hline
        PoEGP & 0.17 & 0.18 & 0.34 \\
        GPoGPE & 0.19 & 0.21 & 0.34 \\
        \hline
    \end{tabular}
    \caption{Gaussian process approximation results for full 24 labels}
}
    \parbox{.45\linewidth}{
        \centering
    \begin{tabular}{|C{2cm}|C{2cm}|C{2cm}|}
        \hline
        Ensemble Type & Labels used & Time Taken (hh:mm:ss) \\\hline
        PoE   & 4   &  00:00:23 \\
        PoE   & 24 &  00:00:37 \\
        GPoE  & 4   &  00:00:23 \\
        GPoE  & 24 &  00:00:40 \\
        \hline
    \end{tabular}
    \label{table:gpensemble-training}
    \caption{Gaussian process ensemble training runtimes for all $5000$ Training points}
}
    \hfill
    \parbox{.45\linewidth}{
        \centering
    \begin{tabular}{|C{2cm}|C{2cm}|C{2cm}|}
        \hline
        Ensemble Type & Labels used & Time Taken (hh:mm:ss) \\\hline
        PoE   & 4   & 00:03:35 \\
        PoE   & 24  & 01:54:47 \\
        GPoE  & 4   & 00:03:31 \\
        GPoE  & 24  & 01:53:30 \\
        \hline
    \end{tabular}
    \label{table:gpensemble-predictions}
    \caption{Gaussian process ensemble prediction runtimes for all $500000$ test points}
}
\end{table}

\begin{figure}
    \includegraphics[scale=0.64]{gpogpe4_allpreds.pdf}
    \caption{Argmax map of generalised product of Gaussian process experts for simplified labels}
    \label{fig:gpogpe4}
\end{figure}

\begin{figure}
    \includegraphics[scale=0.7]{gpogpe23_allpreds.pdf}
    \caption{Argmax map of generalised product of Gaussian process experts for full 23 labels \todo{match colours better}}
    \label{fig:gpogpe23}
\end{figure}

Given that random forests were able to best fit the training data in \Cref{chapsec:deterministic}, it is promising that the the simplified generalised product of Gaussian process expert predictions, whereby the label with the maximum probability at each point is taken to be the `absolute' truth, bears close a resemblance to it. If Gaussian processes were used in practice, there is still the variance that can be used to make better use of the predictions. 

While the product of experts and its generalised counterpart appear to have the comparable performance measured on error alone, the former failed to predict labels $0$ through $2$ in the simple case, and the corresponding labels for the full $24$. The generalised product of experts however, performed quite well, identifying similar habitat regions to the Dirichlet multinomial, as shown in \Cref{chapsec:dm}. Even with these improvements to running time whilst retaining probabilistic information, there is still important functionality missing from these models - the counts of other labels at every point that was available in the original data. The methods explored thus far are not able to deal with such multi-output data, and other models must be used to do this. Moreover, the level of parallelisation used to speed up operations would only be available on server hardware that can become expensive if constantly relied upon. Although scaling Gaussian processes was possible here using hardware and approximation methods, sacrifices in correctness were made. 

To use in live scenarios, such as an AUV with previous bathymetry data collecting new images, the time it would take to run approximations before being able to calculate a desired path would render the vehicle inactive for most of the time. The several minutes achieved by the ensemble methods on the simplified data above was a result of parallelising over $30$ cores and being able to utilise over $200$GB - not the sort of hardware that would be appropriate for a small deep sea vessel. Not to mention, the sort of post-processing required in simplifying labels requires human intervention, and would not be something that can be automated on an expedition into previously unexplored areas. Hence, different methods would be needed that can perform similar tasks that can provide measures of uncertainty in realtime, all while being able to run quickly on hardware with limited capabilities.

\pagebreak
