\documentclass[12pt]{article}
\usepackage[margin=0.7in]{geometry}
\usepackage{multicol}               % Columns
\usepackage{tabularx}               % Extra table functionality
\setlength{\skip\footins}{1.0cm}    % More space between text and footnotes
\usepackage[round]{natbib}          % Round brackets when citep*-ing sources
\usepackage{comment}                % Comments...
\usepackage{titlesec}
\usepackage[parfill]{parskip}       % Newline instead of indentation per paragraph
\usepackage{enumitem}
\setlist{nosep}
\setlength{\columnsep}{0.5cm}       % Separate columns (when used) by 0.5cm
\setcounter{secnumdepth}{4}         % Give paragraphs 'numbers'

\title{A Literature Review in Machine Learning in Benthic Habitat Mapping \\ Research Methods - INFO5993 Assignment 2}
\author{Justin Ting, 430203826}
\date{April 2016}

\begin{document}
\maketitle

            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% INTRO %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
           \section{Introduction}

            Earth's oceans cover 70\% of its surface, but only less than 10\% of the Earth's oceans have been explored to date.~\citep*{NOAA} There have been increasing efforts over the past few decades to map out these unexplored areas to monitor marine ecosystems to be able to track the state of them over time for management, preseveration, etc. purposes. The process used is called benthic habitat mapping, which is the predicting of what exists at the bottom of a body of water. Most recent studies looking to create benthic habitat maps share some basic key steps - acoustic data is used to estimate properties about the surface of the water, which are then mapped to, using machine learning algorithms, \textit{in situ} data such as still images, videos, or samples of the area in question. It is the relationship which is inferred between the different data sets inferred using machine learning techniques that varies between studies.

            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% OVERVIEW %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
           \section{Overview}
            The process of benthic habitat mapping involves three key steps which the large majority of all studies in the area go through.\footnote{http://www.ozcoasts.gov.au/geom\_geol/toolkit/mapoverview.jsp}. In this section, we will give a brief overview of each of these steps, along with common procedures used in them across studies in this area.

            \begin{enumerate}
                \item \textbf{Habitat Characterisation} - extracting properties of the environment such as rugosity (roughness), aspect (direction of slope), depth
                \item \textbf{Habitat Classification} - grouping the raw information about the environment into categories, such as sand, granite, etc.
                \item \textbf{Habitat Mapping} - using classifications with the larger scale bathymetry data to extrapolate habitat maps 
            \end{enumerate}

                                %%%%%%%%%%%%%%% Habitat Characterisation %%%%%%%%%%%%%%%
            \subsection{Habitat Characterisation}
            If we were able to collect high resolution data for the entire ocean's benthos - the job of creating benthic habitats for any given area would be (relatively) trivial. As this is prohibitively expensive, we instead collect large amounts of low resolution data, and small samples of high resolution data (between which we model a relationship). This subsection provides a brief summary of data collected and methods used to do so.


            \paragraph{Remote-sensing data}
            Due to the cost of sea expeditions, it is economically infeasible to have marine vehicles (autonomous or otherwise) explore the entire ocean floor to confirm the ecological properties of all of Earth's benthos. However, we do need to collect sufficiently detailed data of large areas at a time, partiulcarly those of which we are mapping, and for this, remote-sensing data is used. These usually come in the form of acoustic backscatter data, which involes the firing of sound waves towards the benthos, whereby their frequency and strength upon returning is used to deduce the depth at which a particular material was, as well the density of said material (from which a guess at the actual substance can be made - e.g. sand, mud, etc.).

            Multibeam echosounders (MBES) are becoming a more frequently used method of collecting acoustic backscatter data ~\citep*{calvert15} despite older methods involving single beam echo sounders (SBES) being cheaper and easier to segment. This stems from the fact that the reduced cost comes at the expense of (potentially) accuracy, as well as lower resolution data. This is due to SBES' beam angle, i.e. the angle formed by the 2D flattening of the 'cone' shape of the emitted beams, ranging from 15-25$^{\circ}$, whereas MBES' is 0.5-3$^{\circ}$, depending on the particular system~\citep*{cjbrown11}. The difference in angle means that data returned via SBES devices are more 'coarse', reprsenting less accuracy and granularity, whereas that of MBES is more detailed and can present more information. However, there is overhead associated with use of MBES, in that the considerably decreased angles means much more 'overalpping' data which adds complexity to the segmentation process.

            \paragraph{Truthing Data}
            The most common methods to be able to obtain a sufficiently large truthing data set (but still trivially small compared to the area covered by remote-sensing data) are videos or images - though the former still requires post-processing to extract the needed images. The advantage that can be provided here, however, is the redundancy in data points ~\citep*{rattray14}  - but there is extra cost in time required to convert videos into the needed images (pre-proessing before feeding into algorithms for habitat mapping), which is in itself worth of research within the field.~\citep*{lucieer13}

            \paragraph{Other data}
            Other data which is less common, but also used to map habitats, is patterns in the water movement (such as tidal currents, wave action)~\citep*{cjbrown11} in the column of water above the area of benthos being mapped - a feature which has proved to provide useful input in arriving at an accurate benthic habitat map (in addition to sediment analysis).~\citep*{snelgrove94} Other sources such as UNESCO have also verified the importance and significance of using water column correction techniques to obtain more accurate habitat maps, particularly when correlating images with segrass standing crop. \footnote{http://www.unesco.org/csi/pub/source/rs10.htm} 

                                %%%%%%%%%%%%%%% Habitat Classification %%%%%%%%%%%%%%%
            \subsection{Habitat Classification}

            Almost all studies use \textit{in situ} 'truthing' data to complement the acoustic data to be able to build a model between the acoustic data and truthing data (creation of these models are explained in following sections). However, we need to know the labels of this data considering that the final goal is to create a habitat map, where any one habitual zone is given its prospetive label - to do this, we also need to label the clusters of truthing data. These categories may be, for example, 'bedrock covered by discontinuous seagrass cover', 'Maerl interspersed with sand and gravel', 'superficially coarse sand to fine gravel covered by dense patches of seagrass', etc.~\citep*{micallef12}. The two overarching ways to perform this classification are in the form of supervised and unsupervised algorithms.

            Studies have used both supervised and unsupervised methods in clustering the initial data for the training step, though in many cases. Often, there may be large amounts of visual data, beyond that which any human or even team can reasonably, manually cluster - and as such, unsupervised algorithms are first used to create these clusters, after which an expert may be brought in to verify/simplify (or otherwise) the resulting clusters.~\citep*{steinberg11} 

            \paragraph{Supervised methods}
            \textbf{TODO - perhaps condense} using established 'classification' schemes from third party bodies such as EUNIS or the Australian Government's 'Interim Marine and Coastal Regionalisation for Australia'.\footnote{http://www.environment.gov.au/resource/interim-marine-and-coastal-regionalisation-australia-version-33}\\
            maximum likelihood estimation ~\citep*{micallef12} - \textbf{TODO cite the following} - bad, requires data to fit with prescribed probabilistic distribution. not data-driven  - in many cases, pre-speicified Gaussian distribution is bad fit for data \footnote{http://www.analyticbridge.com/profiles/blogs/the-8-worst-predictive-modeling-techniques}\\

            \paragraph{Unsupervised methods}
            \textbf{TODO - perhaps condense} unsupervised methods (k-means clustering ~\citep*{henriques14}) to classify data. - \textbf{TODO find citation for the following} bad - k-means tends to produce circular clusters, doesn't work well with data points that are not a mixture of Gaussian distributions \footnote{http://www.analyticbridge.com/profiles/blogs/the-8-worst-predictive-modeling-techniques}
            henriques14 - custom (?) deterministic method on page 79 - supervised classification, wave model used from (Simoes et al. 2012), multivariate data analysis - similarity profile permutation test, simlarity percentages used to determine a species' contribution to groups, BVstepwise to search for relationships between fauna and environmental variables - used depth, median grain size, \% content of different sediment fractions classified according to Udden/Wentworth scale~\citep*{henriques14}

            \subsection{Map Creation}
            The final step is map creation, which a large portion of papers related to benthic habitat mapping focus on - and also where the most variation occurs in terms of the method used. The various approaches used can be categorised into two broad categories. The first is a top down approach whereby the classification of the habitat characterisation data is validated (or otherwise) with the truthing data, and the second is a bottom up approach where the characterisation data is similarly clustered into classes, but not to directly represent a particular habitat - instead, the aim is to find a relationship between the acoustic data clusters and the truthing data clusters which we can model. Using this model, we can then extrapolate the acoustic data which doesn't have corresponding truthing data to create the habitat map.~\citep*{ahsan11} We will explore this aspect more when looking at how the mapping process has evolved over time and the improvements that it has brought about.

            %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Evolution of Map Creation Methods %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
            \section{Evolution of Map Creation Methods}

                                %%%%%%%%%%%%%%% Birth of habitat mapping %%%%%%%%%%%%%%%
            \subsection{The Advent of Benthic Habitat Mapping}

                                %%%%%%%%%%%%%%% Various Methods %%%%%%%%%%%%%%%
            \subsection{Varying Approaches to Benthic Habitat Mapping}
            Considering that benthic habitat mapping covers a diverse range of discplines, namely "marine biology, ecology, geology, hydrography, oceanography and geophysics"~\citep{cjbrown11}, in addition to statistics and machine learning, it is logical that it would take tremendous effort and vast resources to give each relevant discpline an equal, and large amount of attention within any single study. Different papers can thus rely on the collective findings of others, off which they can launch their own research and dig futher along one particular area, or to follow a different path altogether and evaluate the effectiveness of some new method/etc. In this section, we will look at some of the machine learning techniques (or otherwise) used in the creation of benthic habitat maps, and either conclusions that are drawn about benthic habitat mapping, or particular models and relationships that are validated through multiple studies.

            \paragraph {Non-machine learning} An important study was undertaken in 2001 that employs relatively basic statistical analysis, employing different forms of variance as its main tool of analysis, in contrast to more recent studies favouring machine learning techniques. ~\citep*{kostylev01}, at the time (and in fact, even now) integrated more sources of data together than most other studies undertaken - multibeam bathymetric data, geoscientific data, seafloor photographs, habitat complexity, and relative current strength. Rather than drawing broad conclusions about the effectiveness of a collection of tools in creating habiatat maps, deeper analysis is done on subsets of the data to attempt to clarify some of the complexities and intrinsic properties of benthic habitats and ecosystems themselves. Although little is done to address accuracy of the actual results/map in this paper, it provides value through the analysis of variance and covariance performed on and between different benthic/marine properties, establishing relationships typically taken for granted or ignored. For example, it is established that while sediment type contirubted heavily to a higher taxonomic group count, there was little relationship between sediment type and depth. However, this only indicates that there is no linear relationship between the two, and doesn't necessarily preclude a non-linear relationship between them, perhaps with the inclusion of other parameters as well. In particular, Kostylev establishes that gravel subrates are more abundant with varying taxonomic groups than their sand counterparts.

            \paragraph {Using machine learning}
            \textbf{TODO cover a few papers that improve on MBH using ML}
            hasan14 - (actual model for building map though!) supervised random forest decision trees using two models - first with bathymetry + backscatter mosaic only, and second with angular response derivatives as well (page 4), with extra layers of decreasing importance gradually added to both, with the accuracy of the models assessed using an error matrix, overall actual accuracy, and Kappa coefficient ~\citep*{hasan14}

                                %%%%%%%%%%%%%%% Probabilistic Methods %%%%%%%%%%%%%%%
            \subsection{Gaussian Processes}
            The classifications being made regarding benthic habitats naturally involve uncertainty, as we are still learning the relationship between differnet characteristics of benthos with the varying communities of fauna and flora that reside there. Whilst guessing the most likely class for a particular domain deterministically has its practical applications, it is arguably more \textit{natural} to represent the uncertainty ~\citep{rasmussen06}. As our understanding of marine environments is still quite weak ~\citep{un04}, it is debatable whether deterministic results are always appropriate when being used to make high level management decisions relating to marine environments. It may be more valuable to provide a percentage certainty to decision-makers regarding the information they need to know, as the probabilistic representation arguably provides \textbf{more} information.

            A recent study used probabilistic methods to develop a mapping between the clustered acoustic data to continuous cluster probabilities, as opposed to discrete cluster labels, thus representing the certainty (or otherwise) of the results obtained. ~\citet*{bender12} used Gaussian Processes to extend the probabilistic least squares classifier to retain the information regarding certainty of class membership that exists during the classfication process, rather than discarding it in the traditional method. By evaluating the probabilistic results of PTLSC by comparing its results with the actual cluster probabilities obtained in the classification of the images via an unsupervised variational Dirichlet process model, it was shown that the PTLSC method performed better than a PLSC trained directly on the discrete cluster labels in terms of accuracy, mean squared error, and mean variance as well. This demonstrates that while both PTLSC and PLSC err in their predictions when dealing with the transition different boundaries, by maintaining probabilistic information in the PTLSC, it is able to make slightly better judgements in such cases.

            \textbf{TODO}\textit{make some mention about kernel methods generally being perceived as unscalable}
            However, Gaussian processes involve a matrix inversion process that requires an O($n^3$) operation which does not scale well with large datasets, which traditionally use non-parametric methods (\textbf{TODO cite this}). To overcome this whilst reaping the benefits of Gaussian processes, ~\citet*{bender12} extracted subsets of the original dataset on which to perform analysis - specifically, a sample size of 500 randomly chosen from three Gaussians, of the initial millions of observations. While this (\textbf{TODO - brief explanation of how this is representative}), there is likely information to be gained by being able to use a considerably larger portion of the dataset. To do this, a method would be required to generate sparse covariance matrices through approximations ~\citep*{bickel08}, or use of functions that guarantee sparseness as a property~\citep*{melkumyan09}.

            \textbf{TODO mention use of Gaussian processes in adaptive survey design too}

            \section{Conclusion?}
            we need to better understand "the complexities of coastal system functioning rather than simplifying and scaling down the system into smaller components" ~\citep*{diaz04} - \textbf{NOTE} doesn't really help our point - need to spin this to make it support our point of including uncertainty given that we \textit{still} don't understand all the complexities of marine habitats

\newpage

\bibliographystyle{plainnat}
\bibliography{Bibliography}

\end{document}
