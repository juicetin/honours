\section{Probabilistic Approaches (Single Output)}

In this section, we will add an extra layer of information to our models' outputs - the confidence of the label predictions made. When predictive variance at each point is given, a large variance would indicate a low level of confidence as the predicted value is any within a large range, whereas a small variance indicates a high level of confidence in a prediction, as the possible range of values is only a small one. For this, we need probabilistic models that naturally provide this desired variance in its predictions. In particular, as we saw in \Cref{chapsec:gpc}, Gaussian Process Classification is a good option for this.

\subsection{Gaussian Process Classification}

While f-scores and accuracy are still assessed via $10$-fold cross validation whilst using Gaussian Process Classification, we introduce another metric, Area Under the Receiver Operating Curve, to make use of the fact that the one-vs-all Gaussian Process Classifier provides a \textit{likelihood} of each label's membership at each datapoint. This encapsulates that at any given point, predictions will (almost) never be 100\% certain - every single possible label, however unlikely, will have a probability assigned to it. 

Area Under The Receiver Operating Curve
\todo{TODO}

\todo{show more stratified results (not just even split) to show that even splits did better}

\begin{tabular}{|c|c|c|c|c|}
    \hline
    Type of GP Classification & AUROC & Accuracy & F-score & Labels used\\\hline
    Normal & \todo{TODO} & 0.75525 & 0.47921 & 4 labels \\
    PoEGP & & & & \\
    GPoEGP & & & & \\
    BCM & & & & \\
    \hline
\end{tabular}

\todo{May ditch this table}

\begin{tabular}{|c|c|c|c|c|c|c|c|}
    \hline
    No. points & Type of split & Type of GP & Number of runs & AUROC & Notes & F1-score \\\hline
    500     & Even       & GP     &  10        & 0.86534    &                     &         \\
    500     & Stratified & GP     &  10        & 0.80136    &                     &         \\
    1000    & Even       & GP     &  1         & 0.87626    & Deterministic       & 0.56208 \\
    1000    & Even       & PoEGP  &  5         & 0.80973    &                     & 0.47481 \\
    1000    & Even       & PoEGP  &  200       & 0.80186    &                     & 0.47595 \\
    1000    & Even       & GPoEGP &  5         & 0.80864    &                     & 0.51018 \\
    1000    & Even       & GPoEGP &  200       & 0.80105    &                     & 0.47748 \\
    1000    & Even       & BCM    &  5         & 0.80682    &                     & 0.48167 \\
    1000    & Even       & BCM    &  200       & 0.80421    &                     & 0.48227 \\
    1000    & Even       & GPy    &  1         & 0.87638    & RBF, EP (default)   & 0.57013 \\
    \hline
\end{tabular}
\todo{(look at AUROC/AUC and log probabilities as well)}

\todo{highlight areas with low/high certainty, etc. NOTE - investigate the areas with visually even splits of two labels - e.g. right-side arms of label 1,2, and smaller patches in the bottom left corner of label 0,3 - show that uncertainty about whether those areas are label 1 or 2, 0 or 3 respectively, is (should) be high, and that taking argmax for the sake of visual representation within a single image hides this information}

\todo{(talk about variance and extra probabilistic info to be gained)}

\todo{(maps of 4-label full predictions)}

\todo{(maps of all-label full predictions)}

\subsection{Ensemble Gaussian Process Approximations}

Although Gaussian processes provide the benefit of the possible variance for every prediction, the matrix inversion steps required and their $O(n^3)$ complexity prevents scaling of fitting the model beyond several thousand points (that even on high-end consumer hardware is impractically time-consuming), and predictions an order of magnitude above that. Using the naive Gaussian process, model fitting on the aggregated $4$-label data with $5000$ data points took over a day\todo{(double check this again!)}, with the predictions of the $500,000$ training points taking another day. It would be helpful to note that the formulation of Gaussian process classification used in this study (binary one-vs-all classifiers per label) means the complexity not only scales with number of points, but also in the number of possible labels, with each label requiring another underlying Gaussian process to account for it - for example, data containing $24$ labels would require 24 separate binary Gaussian processes. Due to hardware constraints, the $24$-label cases were not run to completion using the naive approach.

On the other hand, the benefits of Gaussian processes need not be sacrificed on account of this disadvantage of Gaussian processes, as approximations exist to break the original dataset into smaller chunks, allowing parallelisation and model fitting per smaller set of data that is only limited by the available hardware. As seen in \Cref{chapsec:gp-approx}, one such method is to use ensembles of Gaussian processes that allow trivial parallelisation. These experiments were carried out on \textbf{d2.8xlarge} Elastic Container (EC2) instances from Amazon, with the following specifications:

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|}
        \hline
        Instance Type & vCPUs* & Memory(GB) & Physical Processor & Clock Speed \\\hline
        d2.8xlarge & 36 & 244 & Intel Xeon E5-2676v3 & 2.4Ghz\\\hline
    \end{tabular}
    \label{table:ec2specs}
    \caption{Amazon EC2 Instance Machine Specifications}
\end{table}

To illustrate the usefulness of these approximation methods, experiments were run to measure their accuracy and f-scores, but also the time needed to run them particularly compared to naive Gaussian processes and usefulness of the maps - providing a point of reference when looking at Dirichlet multinomials in the next chapter.

The following abbreviations are used for the table below: product of experts (PoE), generalised product of experts (GPoE), Bayesian committee machines (BCM), robust Bayesian committee machines (rBCM).
\begin{table}[H]
    \centering
    \begin{tabular}{|C{3cm}|c|c|C{3cm}|}
        \hline
        Ensemble Type & Data tested & Time Taken (hh:mm:ss) \\\hline
        PoE   & 5000 points, 4-labels   &  00:00:23 \\
        PoE   & 5000 points, 24-labels  &  00:00:37 \\
        GPoE  & 5000 points, 4-labels   &  00:00:23 \\
        GPoE  & 5000 points, 24-labels  &  00:00:40 \\
        BCM   & 5000 points, 4-label    &  00:00:24 \\
        rBCM  & 5000 points, 4-label    &  00:00:22 \\
        \hline
    \end{tabular}
    \label{table:gpensemble-training}
    \caption{Gaussian process ensemble training runtimes}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|C{3cm}|c|c|C{3cm}|}
        \hline
        Ensemble Type & Data tested & Time Taken (hh:mm:ss) \\\hline
        PoE   & 500000, 4 labels    & 00:03:35 \\
        PoE   & 500000, 24 labels   & 01:54:47 \\
        GPoE  & 500000, 4 labels    & 00:03:31 \\
        GPoE  & 500000, 24 labels   & 01:53:30 \\
        BCM   & 500000, 4-label     & 00:03:34 \\
        BCM   & 500000, 24-label    & 01:52:49 \\
        rBCM  & 500000, 4-label     & 00:03:38 \\
        rBCM  & 500000, 24-label    & 01:52:03 \\
        \hline
    \end{tabular}
    \label{table:gpensemble-predictions}
    \caption{Gaussian process ensemble prediction runtimes}
\end{table}

In contrast, the same experiments run with a single Gaussian process classifier, but only on a subset of the data, displays the vast difference in run times.
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|C{3cm}|}
        \hline
        Data tested & Operation performed & Time Taken (hh:mm:ss) \\\hline
        1000 points, 4 labels & Model fitting & 00:01:09 \\
        2000 points, 4 labels & Model fitting & 00:04:29 \\
        3000 points, 4 labels & Model fitting & 00:09:02 \\
        4000 points, 4 labels & Model fitting & \\
        5000 points, 4 labels & Model fitting & \\
        1000 points, 24 labels & Model fitting & \\
        2000 points, 24 labels & Model fitting & \\
        3000 points, 24 labels & Model fitting & \\
        4000 points, 24 labels & Model fitting & \\
        5000 points, 24 labels & Model fitting & \\
        500000 points, 4 labels (1000 training) & Predictions & 00:06:23 \\
        500000 points, 4 labels (2000 training) & Predictions & 00:01:57 \\
        \hline
    \end{tabular}
    \label{table:gpensemble-results}
    \caption{Gaussian process runtimes}
\end{table}

\todo{(error/performance table)}

\begin{figure}[H]
    \includegraphics{gpogpe4_allpreds.pdf}
    \caption{Argmax map of Generalised Product of Gaussian Process Experts for simplified labels}
    \label{fig:gpogpe4}
\end{figure}
\begin{figure}[H]
    \includegraphics{gpogpe23_allpreds.pdf}
    \caption{Argmax map of Generalised Product of Gaussian Process Experts for full 23 labels \todo{match colours better}}
    \label{fig:gpogpe4}
\end{figure}

Given that random forests were able to best fit the training data in \Cref{chapsec:deterministic}, it is promising that the the simplified generalised product of Gaussian process expert predictions, whereby the label with the maximum probability at each point is taken to be the `absolute' truth, bears close resemblance. Of course, were such data to be used in practice, there is still the variance that can be used to make better use of the predictions. For example, by being able to generate a map of the entire Scott Reef in under 5 minutes for the basic case, an automated underwater vehicle on a surveying expedition could run processing in real-time to determine areas of lowest confidence to `fill in the gaps'.

...

Even with these improvements to running time whilst retaining probabilistic information, there is still important functionality missing from these models - the counts of other labels at every point that was available in the original data. The methods explored thus far are not able to deal with such multi-output data, and other models must be used to do this. This is the motivation for the following chapter.
