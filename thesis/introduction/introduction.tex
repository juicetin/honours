\chapter{Introduction}

Earth's oceans cover 70\% of its surface, but only less than 10\% of the Earth's oceans have been explored to date\footnote{Oceanservice.noaa.gov. (2016). How much of the ocean have we explored?. [online] Available at: http://oceanservice.noaa.gov/facts/exploration.html}. There have been increasing efforts over the past few decades to more efficiently map out these unexplored areas to monitor marine ecosystems to be able to track the state of them over time for reasons such as management and preseveration purposes. The process used is called benthic\footnote{Benthic refers to things relating to the bottom of the ocean, i.e. the ocean floor} habitat mapping and involves generating predictive maps of different habitat types at the ocean floor. Most efforts to create benthic habitat maps share some basic key steps - acoustic data is used to estimate properties about the surface of the benthos, then its relationship with other `truthing' data such as images or video are inferred. 

Early attempts at this understanding and modeling these relationships would have been expert-driven where any map created by an individual or individuals would be fully subject to their personal biases and knowledge. With the lack of better tools available, this was the best option - until machine learning algorithms began to be formulated more formally, and applied to real world problems. Thus, deterministic methods such as random forests and support vector machines made their debut, and enabled predictions on large sets of potentially complex data that would have previously proved to be a considerable challenge or even impossible when done manually, and would have required major compromises. 
However, considering that the creation of habitat maps would often be for conservation of marine management purposes, it would be important to be able to associate any decisions being made as a result of using the predictive maps with a quantifiable risk factor. This amount of `risk' would correspond to a probabilistic machine learning model that can state the confidence of its predictions, something that derministic methods cannot do.

Gaussian processes are a complex model that can provide this information - providing both the probability of every label being the `correct' one at a certain data point, as well as the variance on each of these predictions. Due to the $O(n^3)$ matrix inversion steps involved in them, the time that would be required to fit a Gaussian process to more beyond several thousand points would be astronomical and impractical for any real world applications. To account for this, there exist approximations such as product of experts and Bayesian committee machines that allow levels of parallelism of the model (that are inapplicable to the basic Gaussian process) that are (almost) bounded only by hardware. 

These methods so far ignore the fact that asynchronously collected bathymetry and image data will often not correspond to one another in a $1:1$ manner (as is the case in this study), with each bathymetry point likely related to several images. This results in a dataset where for each coordinate with bathymetry data, the labels across several labels are attributed to it, or in other words, a vector of label counts. As the aforementioned models do not handle such data, approximations were made where these vectors were simplified down to a single label, throwing out a majority of the information that was collected. To actually fully utilise these vector counts, a method is needed that can model this data appropriately - such as the Dirichlet Multinomial. This allows predictions to be made over category counts, providing normalised predictions at each point, not only correctly interpreting the data in contrast to other methods used in benthic habitat mapping, but also allowing properties of the benthos such as biodiversity and co-habitation to be directly inferred from the data, not requiring additional post-processing steps.

% It is the relationship which is inferred between the different data sets inferred using machine learning techniques that varies between studies. A considerable portion of such studies are shown to use deterministic methods to predict a label for any given coordinate such as Random Forests and Support Vector machines (SVMs), whilst more recent ones make use of more informative methods such as Gaussian Processes, providing a distribution over all possible labels given any data point.

\section{Contribution}

The main contributions of this thesis are twofold. Fistly, an investigation into current probabilistic methods is performed. Gaussian proceses are a state-of-the-art method in this regard, although the implementations of them available in open source libraries and the like are currently unable to work with datasets larger than several thousand points without hitting computational limits, which makes them impractical for real-world use, if, for example, trying to fit a model for data prediction alone for several tens of thousands of points could take weeks, if not longer. This work seeks to investigate whether recent innovations in the field are able to improve performance sufficiently that it can be used on larger datasets, and in particular, to the specific field of benthic habitat mapping.

The second aim is to explore the use and modelling of information where input data does not only have a single label such as `sand', but could have a category count over labels, such as $7$ counts of sand, $3$ counts of coral, and $1$ count of rhodliths, also representible as a normalised distribution $[0.7, 0.3, 0.1]$. Such data is collected as a result of automated underwater vehicles (AUV) collecting  images at the ocean floor at a density of, for example, one every $10m^2$, whereas bathymetry data is collected from the ocean surface at a lower rate, such as $50m^2$. This means there can be more than one image, and hence habitat label per bathymetry data point, where any single image is `assigned' to the nearest bathymetry data point. While this can be used with single-output machine learning algorithms, it involves simplifying every data point's label distribution to only the most dominant label - in the aforementioned example, $[0.7, 0.3, 0.1]$ would be simplified to sand. Although this allows common and readily available methods to be used, it also discards a considerable amount of otherwise useful information. To fully utilise all this information, use of the Dirichlet Multinomial distribution to model the data is explored, along the other information that can be gathered from it such as measurements of biodiversity and entropy to represent confidence in predictions.
% \todo{(need to rewrite most of this. investigating probabilistic methods and improving their performance as well)} The main contribution of this thesis will be to explore how to use data where a single data point does not have only one label exclusively, but instead corresponds to a tally of each possible label. For example, a particular 5m x 5m area in the benthos may be an even mix of both sand and coral, but in previous literature, the data was simplified such that whichever label occurred more frequently regardless of how small the margin would be the single label assigned to that point. This results in a very coarse approximation even when using Gaussian Processes attempts to model the uncertainty/uncertainty with its predictions at each point (but ultimately only provides a single, final prediction). To alleviate this and provide a richer set of information, we explore the use of Dirichlet Multinomials, which provides a distribution of each label that represents something entirely different. Whereas in a Gaussian Process, each label is assigned the probability of being the correct one, the output of a Dirichlet Multinomial Regressor provides the distribution of the frequency of labels in a particular space itself. See section \todo{GP vs DM} for an illustrative example on how results would differ in practice between the two methods.

\section{Motivation}
The motivation for exploring the aforementioned technical aspects of benthic habitat mapping is to explore how the process of benthic habitat mapping can be performed more economically while also providing richer information than has generally been the case in such efforts, and also automating as much of the process as possible. To break down the three aims mentioned - the economic aspect comes into play in terms of density at which the data is collected. Naturally, collecting more data would cost more money - being able to work with coarser data where multiple images may correspond to single bathymetry data point using a technically sound method that can make full use of the data has the potential to cut down on the need to collect huge amounts of data. Richer information can be obtained both from the probabilistic component of Gaussian processes as well as the Dirichlet distribution's entropy from the Dirichlet multinomial. This is necessary because any actions taken to manage or conserve benthic habitats inherently comes with a level of risk whether it be environmental or economical, and quantifying this risk would require that at the very least, the mathematical models used to form predictions need to be able to state their confidence.

% \todo{(motivation for speeding up GP?)} The motivation behind assessing the effectiveness and advantages of such a method are that they inherently tie in with lower resolution data, particularly when a single bathymetry points corresponds to a large enough area such that one would expect a mix of different labels. This is advantageous because we want to be able to re-sample data from any given site periodically (for example, every 3-4 years) whilst being economically efficient. This naturally lends to lower resolution data, meaning that summarising large areas to a single label would theoretically be throwing away a majority of the information contained in bathymetry and image data.

\section{Outline}

We will first review some of the existing literature in \Cref{chap:relatedwork}, on the collection of bathymetry and image data briefly, then on deterministic approaches to benthic habitat mapping to date, such as logistic regression, and random forests, and their performance on varying types of benthic environments. This is in contrast to the more informative probabilistic and multi-output approaches that will be explained in \Cref{chap:gps} and \Cref{chap:dms}, where we look at the mathematical background behind Gaussian processes and Dirichlet multinomial regression. In \Cref{chap:experiments}, we then apply the techniques explained in the previous chapters and observe their performance, points of interest, as well as how the information obtained differs to methods visited in explored in the related works. At the end of this chapter, the limitations inherent in the experiments is also explained, with suggestions on how they could have been avoided. The study is then concluded and summarised in \Cref{chap:conclusion}, discussing the possible impact of this work, as well as areas of the study that could be explored in further detail as possible future work.
