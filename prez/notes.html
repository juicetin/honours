<p>=================== script =================== # Motivation ~3 minutes Given that most of the Earth is made up of our oceans and its role in guiding the global climate and influencing life on Earth in general, it is crucial that we are able to constantly monitor the state of our oceans. However, it's simply not feasible to send robots to comprehensively capture detailed images of the entire ocean - and so we need benthic habitat mapping, where a benthic zone refers to the bottom of a body of water such as a lake bed, or the case that we are interested in, the ocean floor. Modern technology has allowed us to use autonomous underwater vehicles, or AUVs for short, such as the Australian Centre for Field Robotics' Sirius to collect images, while tools such as multibeam echosounders allow ships to collect bathymetry data telling us the depth of a particular position, allowing inference of other information as well such as slope and roughness.</p>
<p>Because of the comparatively high cost of operating AUVs, the area over which image data is collected can only be a small fraction of the bathymetry data collected. For example, the bathymetry data used in this study contains over 2.7 million points, whereas the images collected only corresponded to 16,502 bathymetry data points. Numerous images are collected within the area covered by each bathymetry data point though - this will be explained in more detail later. We can then model the relationship between the images and bathymetry data where they both exist and apply this model to the areas where only bathymetry data exists - generating a predictive map that removes the need to send AUVs along every inch of the entire ocean floor. Ideally, we want this procedure to be performed directly on the robots during missions, as an understanding of its surroundings can allow it to more efficiently map out the area it is exploring, further reducing the costs of these expensive expeditions. Thus, we not only want to generate maps, but be able to do it as quickly and efficiently as possible by using as much of the available data as possible, if not all of it. With a high level understanding of benthic habitat mapping, let's now look at some of the existing literature.</p>
<h1 id="related-works-2.5-minutes">Related Works ~2.5 minutes</h1>
<p>The earliest efforts would have involved sending out divers to manually collect sediment samples that experts then used along with any existing knowledge they had to draw very crude habitat maps for an area. Once technology evolved to scale beyond manual sampling of habitats, the amount of data available allowed machine learning techniques to be applied to perform predictions. Methods such as Random Forests are used very often in benthic habitat mapping studies, but in machine learning classification problems in general, due to their robustness. Just briefly, they work by using multiple decision trees that operate on random subsets of the available data, taking the most commonly occurring prediction, or the mode across the DTs, as the final prediction.</p>
<p>However, as I mentioned earlier, the point of all these predictive maps is to gain an understanding of our ocean's habitats to inform us of the sort of conservation efforts that are required - and being able to quantify the uncertainty, and hence the risk involved in any planned actions is crucial - and is not something that deterministic methods such as random forests is able to provide us.</p>
<p>One of the few works in benthic habitat mapping that use probabilistic predictions was by Bender, Williams, and Pizarro in 2012, where the use of Gaussian processes showed that using probabilistic labels as input resulted in a lower mean error and variance than using deterministic labels, but in both cases still being able to quantify the variance of the predictions made, corresponding to a level of certainty. But there was a bottleneck that the study faced with the data used, as the covariance matrix inversions scale cubically, making predictions beyond several thousand points infeasible. As a result, only 2500 points were used for the experiments in the study.</p>
<h1 id="approach---gp-approx-1.5m">Approach - GP Approx (~1.5m)</h1>
<p>On this note, the first thing we aimed to do was explore the use of methods to overcome the data restrictions of Gaussian processes. Of the possible approximation methods, we considered two ensemble ones - the product of Gaussian process experts, and generalised product of Gaussian process experts. These function by using a collection of separate Gaussian processes that each model unique subsets of the dataset that are then multiplied back together and weighted by their variance as an approximation to the parameters of the Gaussian over all the data. Let's say we had 20-thousand points and 100 separate GPs - instead of having to invert a single 20-thousand by 20-thousand matrix, we would only need to invert 100 separate 200 by 200 ones - a difference in complexity of four orders of magnitude.</p>
<h1 id="approach---intro-to-multi-output-methods-1m">Approach - Intro to multi-output methods (~1m)</h1>
<p>However, these GP approximations still result in a simplification of the training data that means we are discarding valuable information even before any machine learning happens - because we are only assigning a single label to each bathymetry data point. As you can see here though, the collected data is richer than that - each of the blue dots are bathymetry points, and the coloured squares around them are images. You can see that three of the five cells have a mix of labels, something that even probabilistic methods can't accurately model, as every data point now has multiple outputs in the form of counts of each label, or the equivalent distribution per label.</p>
<h1 id="approach---dirichlet-multinomial-regression">Approach - Dirichlet Multinomial Regression</h1>
<p><TODO> So, onto the result to see how they performed.</p>
<h1 id="results---gp-vs-dm-vs-deterministic-methods-0.5m">Results - GP vs DM vs Deterministic Methods (~0.5m)</h1>
<p>Something to keep in mind here is that for the Gaussian process and Dirichlet multinomial, to be able to compare them their rich outputs to the deterministic methods, the most probable and most frequent labels per point were taken respectively. We can see that the Gaussian process performed the best across both sets of labels in both accuracy and f-scores, while SVM performed the worst in most cases.</p>
<h1 id="results---deterministic-maps-0.5m">Results - Deterministic Maps (~0.5m)</h1>
<p>Each of the predictive maps generated by the deterministic methods varied quite a lot in terms of the clusters of labels that were present as well as where they occurred - but without any indication of the likelihood of these predictions, it is hard to say that one is better than the other, particularly as the k-nearest neighbour and random forest scores were consistently very close.</p>
<h1 id="discussion---gp-vs-gpogpe">Discussion - GP vs GPoGPE</h1>
<ul>
<li>some similarities</li>
<li>locations of key clusters different</li>
<li>GP needs to learn a single set of hyperparameters for the training data</li>
<li>GPoGPE is more flexible, many GPs learning different HPs for different sections of data (corresponding to different regions)
<ul>
<li>reflected in results - though GPoGPE was on paper worse with cross-val scores, it was much closer to the DM maps</li>
</ul></li>
</ul>
<h1 id="discussion---gaussian-process-variance">Discussion - Gaussian process variance</h1>
<ul>
<li>1 standard deviation at most likely labels at each point</li>
<li>we are now able to see how confident predictions were by their spread over possible values</li>
<li><todo - need to see GPoGPE results to say anything for sure here></li>
</ul>
<h1 id="discussion---dirichlet-multinomial">Discussion - Dirichlet Multinomial</h1>
<ul>
<li>able to see more visual richness even in the argmax graph taking MAP approximation</li>
</ul>
<h1 id="discussion---mcmc">Discussion - MCMC</h1>
<ul>
<li>MAP only takes max approx from posterior distribution</li>
<li>we can take draws of weights using MCMC and see if the maps are mostly in agreeance</li>
<li><gif></li>
</ul>
<h1 id="discussion---entropy">Discussion - Entropy</h1>
<ul>
<li>There's this level of uncertainty due to the possible weights</li>
<li>Use entropy to determine the less certain areas</li>
<li>Quantify which areas are certain or not</li>
</ul>
<h1 id="discussion---biodiversity">Discussion - Biodiversity</h1>
<ul>
<li>DM can be used to determine biodiverse areas</li>
<li>e.g. this blank area here with no biodiversity if you remember the bathymetry depth map, is very deep and is just all sand</li>
<li>this higher point though, is a large raised area rich with different types of coral</li>
</ul>
<p>================= end script =================</p>
<h1 id="motivationobjective-of-own-work">Motivation/Objective of own work</h1>
<ul>
<li>1 minute</li>
<li>Benthic habitat mapping studies generally use deterministic, single-output methods</li>
<li>For people involved in manging or conserving ocean habitats, they need to be able to quantatively assign risk to any action they may need to take
<ul>
<li>this allows appropriate resources dedicated to backup actions/fallbacks that may need to be put into place</li>
<li>an all-or nothing approach is not an option in terms of any sort of environmental conversation</li>
</ul></li>
<li>probabilistic methods (GPs) have a time constraint</li>
<li>need for multi-output</li>
<li>because of the way data # Background literature</li>
<li>1 minute</li>
<li>go through some basic deterministic approaches used in the past - SVMs, RFs, even LR</li>
<li>mention works that have touched on use of GPs - Bender's 2012 work # Outlining of own approach/method/algorithm</li>
<li>3 minutes</li>
<li>applying GP approximations to mapping to see how they perform relative to deterministic methods</li>
<li>multi-output by not simplifying the data # Main results of own project</li>
<li>2 minutes # Discussion/Evaluation/Contribution of own work</li>
<li>2 minutes # Conclusion and Future Work</li>
<li>2 minutes</li>
</ul>
<h1 id="motivation">Motivation</h1>
<p>Before we delve into it, I'll give a brief primer on what the focus of this study, benthic habitat mapping is. The benthic zone refers to lowest layer of a body of water, such as lake bed or the ocean floor. So, looking at this image, we can get an idea of what benthic habitat mapping is using a simple example. First, bathymetry data is collected over an extensive area so that we know the topography of the benthic zone in question, allowing us to infer other information such as roughness and slope - these are indicated by the blue and orange dots. For a subset of the area, we then collect data that allows us to verify the true habitats - in this case, the dotted black line representing an autonomous underwater vehicle taking images at each black square. By modelling a relationship between the bathymetry data and the images collected where both exists, we can then apply it to the bathymetry data that does not have corresponding labels, to predict the habitats in those areas. The reason for not simply collecting the image data for the entire area is that it is prohibitively expensive, and it's easy to imagine why it's not economically feasible to do this for the entire ocean. By creating these maps, different agencies and government bodies can be better informed and have a better understanding of the state of our oceans, and be able to take appropriate actions to manage and conserve them based on environmental changes.</p>
<p>Here, you can see what a data collection expedition may look like - for each bathymetry data point, several labels are attributed to it as a result of multiple images being taken in that region. The majority of studies use deterministic, single-output methods that require simplifying each bathymetry data point to a single label, as indicated by the coloured circles that take the most frequently occurring label within it.</p>
<p>However, this dilutes the information in a way that results in data loss - looking at the actual counts of different habitat labels in the range of each bathymetry point, there is actually an underlying distribution at each point that is not accounted for.</p>
<p>As a result, methods that simplify the information in this way don't quite capture the full picture, and this study aims to evaluate methods not explored in existing literature to benthic habitat mapping. The first approach is to maintain the of simplification of labels, but to instead provide probabilistic output so instead of dealing with absolutes, saying a particular area is simply 'definitely sand', a probability range is given for every possible label. The second is to directly work with the distributions of labels at each point, performing multi-output predictions that do not simplify the labels counts per habitat in the original data down to only a single label.</p>
<p>The reason we want to do this is to be able to quantify the risk associated with any action that could be taken as a result of trusting the data presented by predictive habitat maps - and to also not discard any underlying information that can be used in the rich data source.</p>
<h1 id="related-literature">Related Literature</h1>
<p>The large portion of related literature in benthic habitat mapping use the same few deterministic methods combining bathymetry data and image data - for example, random forests and support vector machines. Based on the cross validation results, both appear to generally perform quite well, for example in Wahidin's work in 2015 where both are used, but the predictive maps over the area studied aren't shown, so it's uncertain whether they are realistic or not despite cross-validation scores - something that we'll see in the experiments run in this study.</p>
<p>There have been some works in the past several years as well that have started to look at probabilistic benthic habitat mapping - in 2012, Bender, Williams, and Pizarro used subsamples of data and applied Gaussian process classification that allowed them to determine the variance of their predictions. By comparing the results of using label probabilities vs absolute labels, they showed that using the probabilistic information resulted in a lower error and variance than when labels were simplified to a single definitive value. The reason that only a subsample of the data was used was dueo the Gaussian process' O(n^3) complexity from the need for matrix inversions, and using all the training data would have resulted in impractically long training times, particularly when performing 10-fold cross validation.</p>
<h1 id="approach">Approach</h1>
<p>The first area that we explore is how to make full use of the datasets when using probabilistic methods - and not be constrained by Gaussian process' practical limit of several thousand data points due to the matrix inversions. To do this, we consider two related methods of approximation that allow the breaking down of data into smaller problems - namely, product of experts and generalised product of experts. What these methods do is they train separate Gaussian processes on small subsets of the data, for example, lots of 200, and combine all their results, weighting them by their variance, where the lower variance experts are given a higher weighting, and the higher variance ones a lower weighting. This study uses the product of experts and generalised product of experts approximation methods - the difference between them being that while the product of experts uses the variances for weighting directly. The problem with this is that the precisions can add up and cancel each other out - the generalised version aims to reduce this problem by dividing the weight-values by the number of experts. This effect can be partially seen below - for the product of experts, areas between points without data are overconfident, almost having no variance at all - whereas for the generalised version , the same areas still state have a variance interval.</p>
<p>The other method that was investigated is Dirichlet multinomial regression, which allowed us to directly model the label distributions that were present in the original data. TODO - brief explanation of dirichlet multinomial - briefly explain multinomial, then dirichlet and how it is s a distribution of weights of the multinomial distribution</p>
<h1 id="results">Results</h1>
<h2 id="gp-vs-deterministic">gp vs deterministic</h2>
<p>Looking at a comparison of deterministic results with Gaussian processes on this dataset, we can see that the standard GP is able to outperform the most common classification methods used, including those often seen in benthic habitat mapping studies. The approximations don't fare as well, but as we will see, this isn't necessarily indicative of real world performance.</p>
<h2 id="det-maps">det maps</h2>
<p>The maps generated on the full query dataset by fitting the training data are quite different - only the logistic regressor displays notable areas of label 0, whereas random forests display the most diversity throughout the region, with k-nearest neighbours failing to detect anything other than label 3, which was also the most dominant label in the training data.</p>
<h2 id="gp-maps-and-gpogpe">gp maps (and gpogpe)</h2>
<p>The full predictive maps for the standard GP and generalised product of GP experts were quite different, but as we see later, the approximation method's maps are in fact closer to the Dirichlet multinomial's that properly model the original data without simplfying the original labels counts to a single label.</p>
<h2 id="gp---probabilities-and-variance">gp - probabilities and variance</h2>
<p>Looking at the underlying probabilities of the most likely labels at each point, we can see that they all sit in the 62-69% range, which is not very high - with the deviation of each one being quite large as well. This points to the fact that the GP is not very certain about its predictions, and this could likely be due to the biodiversity present in Scott Reef - something we will look at in the next section, when using Dirichlet multinomial regression</p>
<h2 id="dm-maps">DM maps</h2>
<p>Using Dirichlet multinomial regression, we can see how often labels occur and their densities separately - label 3 being the most common, occurring at a rate of more than 50% for over 75% of the points throughout Scott Reef. We can see that label 1 dominates the reef for small section on the far left, with a mix of labels 0, 1, and 3 in the upper-left region.</p>
<h2 id="dm-entropy">DM entropy</h2>
<p>Using the DM, we can also visualise how 'likely' the predictions are using the entropy of each point across the label distribution of every label. This entropy heatmap shows that the entropy for the predictions are quite low throughout the Scott Reef, with no noticable are of high entropy. The particularly dark purple areas indicate a very low entropy, and comparing back to the predictions for the simplified labels, these were the same areas where there were noticable amounts of a consistent mix of labels.</p>
<h2 id="timing-of-gp-vs-dm">timing of GP vs DM</h2>
<p>Given that the aim of approximation was to overcome the data limitations of a standard GP, we would want to see the different in run times for the a GP and its approximation, as well as the Dirichlet multinomial as well. Training of a generalised product of GP experts is considerably faster than a GP, as matrix inversions are limited to matrices of 200x200, compared to the GP that needs to invert a 4700x4700. Because of the overhead of a large number of experts though, as the matrix operations can be optimised, predictions for the approximation take longer than for the standard one, but is still much faster. The Dirichlet multinomial, on the other hand, does not involve the expensive steps that GPs do and only need top optimise over their parameter space - their speed is only in the seconds and are instantaneous compared to either the GP or its approximations.</p>
<h1 id="discussion">Discussion</h1>
<p>As seen earlier, the cross validation scores did not directly reflect in the quality of the predictive maps - one contributor to this may have been the fact that there was a significant class imbalance. However, without the opinion of an ecological expert, it is difficult to determine whether the cause may have been intrinsic in the data - and whether in the case of Scott Reef, bathymetry data and derived properties such as roughness and slope alone were enough to be able to clearly differentiate between habitats. The variance on the predictions suggest that this may be the case, as it was quite high for each of the most probable labels throughout the query space.</p>
<p>The resultant maps are also difficult to evaluate both quantitatively and qualitatively without input from marine biologists or similar on whether the patterns that emerge between habitats and their co-existence are plausible or not. Because of these missing sources of knowledge, the analysis on the results are restricted to interpreting the available data and predictions numerically, and only being able to make broad observations as to what they imply.</p>
<h1 id="conclusion">Conclusion</h1>
<p>In this thesis, we used approximation methods for Gaussian processes to show that they were a viable approach to scale probabilistic methods to larger datasets, without a loss of quality or usefulness in the resultant data compared to other methods. We then proposed fully utilising the label counts over all the habitats in the original data by using Dirichlet multinomial regression, so that predictions provided a distribution over labels, rather than a single label alone. This output then allowed easy extraction of underlying information of the habitat such as biodiversity that would require more expensive post-processing with other single-output classification methods.</p>
<h1 id="things-to-keep-in-mind">Things to keep in mind</h1>
<ul>
<li>check default settings for deterministic methods to be able to answer any questions about the scikit learn algo configurations</li>
</ul>
